{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to probaforms Probaforms is a python library of conditional Generative Adversarial Networks, Normalizing Flows, Variational Autoencoders and other generative models for tabular data. All models have a sklearn-like interface to enable rapid use in a variety of science and engineering applications. Implemented conditional models Variational Autoencoder (CVAE) Wasserstein GAN (WGAN) Real NVP Installation pip install probaforms or git clone https : // github . com / HSE - LAMBDA / probaforms . git cd probaforms pip install - e . or poetry install Basic usage (See more examples in the documentation .) The following code snippet generates a noisy synthetic data, fits a conditional generative model, sample new objects, and displays the results. from sklearn.datasets import make_moons import matplotlib.pyplot as plt from probaforms.models import RealNVP # generate sample X with conditions C X , y = make_moons ( n_samples = 1000 , noise = 0.1 ) C = y . reshape ( - 1 , 1 ) # fit nomalizing flow model model = RealNVP ( lr = 0.01 , n_epochs = 100 ) model . fit ( X , C ) # sample new objects X_gen = model . sample ( C ) # display the results plt . scatter ( X_gen [ y == 0 , 0 ], X_gen [ y == 0 , 1 ]) plt . scatter ( X_gen [ y == 1 , 0 ], X_gen [ y == 1 , 1 ]) plt . show () Support Home: https://github.com/HSE-LAMBDA/probaforms Documentation: https://hse-lambda.github.io/probaforms For any usage questions, suggestions and bugs use the issue page , please. Thanks to all our contributors","title":"Home"},{"location":"#welcome-to-probaforms","text":"Probaforms is a python library of conditional Generative Adversarial Networks, Normalizing Flows, Variational Autoencoders and other generative models for tabular data. All models have a sklearn-like interface to enable rapid use in a variety of science and engineering applications.","title":"Welcome to probaforms"},{"location":"#implemented-conditional-models","text":"Variational Autoencoder (CVAE) Wasserstein GAN (WGAN) Real NVP","title":"Implemented conditional models"},{"location":"#installation","text":"pip install probaforms or git clone https : // github . com / HSE - LAMBDA / probaforms . git cd probaforms pip install - e . or poetry install","title":"Installation"},{"location":"#basic-usage","text":"(See more examples in the documentation .) The following code snippet generates a noisy synthetic data, fits a conditional generative model, sample new objects, and displays the results. from sklearn.datasets import make_moons import matplotlib.pyplot as plt from probaforms.models import RealNVP # generate sample X with conditions C X , y = make_moons ( n_samples = 1000 , noise = 0.1 ) C = y . reshape ( - 1 , 1 ) # fit nomalizing flow model model = RealNVP ( lr = 0.01 , n_epochs = 100 ) model . fit ( X , C ) # sample new objects X_gen = model . sample ( C ) # display the results plt . scatter ( X_gen [ y == 0 , 0 ], X_gen [ y == 0 , 1 ]) plt . scatter ( X_gen [ y == 1 , 0 ], X_gen [ y == 1 , 1 ]) plt . show ()","title":"Basic usage"},{"location":"#support","text":"Home: https://github.com/HSE-LAMBDA/probaforms Documentation: https://hse-lambda.github.io/probaforms For any usage questions, suggestions and bugs use the issue page , please.","title":"Support"},{"location":"#thanks-to-all-our-contributors","text":"","title":"Thanks to all our contributors"},{"location":"examples/2-time-series-gan/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); 3 \u0444\u0435\u0432\u0440\u0430\u043b\u044f 2021 \u0433. \u041c\u043e\u0441\u043a\u0432\u0430 \u0422\u0443\u0442\u043e\u0440\u0438\u0430\u043b \u041f\u043e \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u044e \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0439 \u0438 \u0442\u043e\u0447\u0435\u043a \u0440\u0430\u0437\u043b\u0430\u0434\u043a\u0438 \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u044b\u043c\u0438 \u043c\u043e\u0434\u0435\u043b\u044f\u043c\u0438 Content Change-points and Anomalies Conditional GAN Conditional BiGAN Change-point and Anomalies Detection Based on Prediction Errors Change-point and Anomalies Detection Based on BiGAN Reconstruction Errors Additional Resources Anomalies vs Change-Points % matplotlib inline import matplotlib.pyplot as plt import numpy as np import pandas as pd Data Reading Yahoo S5 - A Labeled Anomaly Detection Dataset, version 1.0(16M) Automatic anomaly detection is critical in today's world where the sheer volume of data makes it impossible to tag outliers manually. The goal of this dataset is to benchmark your anomaly detection algorithm. The dataset consists of real and synthetic time-series with tagged anomaly points. The dataset tests the detection accuracy of various anomaly-types including outliers and change-points. The synthetic dataset consists of time-series with varying trend, noise and seasonality. The real dataset consists of time-series representing the metrics of various Yahoo services. Link: https://webscope.sandbox.yahoo.com/catalog.php?datatype=s&did=70 #!wget https://raw.githubusercontent.com/HSE-LAMBDA/OpenTalks2021_tutorial/main/data/Yahoo_S5/A4Benchmark/A4Benchmark-TS12.csv #!wget https://raw.githubusercontent.com/HSE-LAMBDA/OpenTalks2021_tutorial/main/data/Yahoo_S5/A4Benchmark/A4Benchmark-TS37.csv #!wget https://raw.githubusercontent.com/HSE-LAMBDA/OpenTalks2021_tutorial/main/data/Yahoo_S5/A1Benchmark/real_57.csv ! wget https : // raw . githubusercontent . com / HSE - LAMBDA / OpenTalks2021_tutorial / main / data / Yahoo_S5 / A1Benchmark / real_61 . csv --2021-02-02 19:12:08-- https://raw.githubusercontent.com/HSE-LAMBDA/OpenTalks2021_tutorial/main/data/Yahoo_S5/A1Benchmark/real_61.csv Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133 Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 13774 (13K) [text/plain] Saving to: \u2018real_61.csv.2\u2019 real_61.csv.2 100%[===================>] 13.45K --.-KB/s in 0s 2021-02-02 19:12:08 (94.5 MB/s) - \u2018real_61.csv.2\u2019 saved [13774/13774] data = pd . read_csv ( \"real_61.csv\" , sep = ',' ) data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } timestamp value is_anomaly 0 1 2 0 1 2 14 0 2 3 4 0 3 4 2 0 4 5 42 0 # get a time series Y = data [[ 'value' ]] . values plt . figure ( figsize = ( 12 , 5 )) plt . plot ( Y , label = 'True' ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . grid ( b = 1 ) plt . show () Preprocessing # scaling from sklearn.preprocessing import StandardScaler Y = StandardScaler () . fit_transform ( Y ) plt . figure ( figsize = ( 12 , 5 )) plt . plot ( Y , label = 'True' ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . grid ( b = 1 ) plt . show () Autoregressive Model for Time Series Forecast Consider y_1, y_2, ..., y_i, ..., y_N are observations of a time series. Autoregressive Model for the time series forecast assumes the following: \\hat{y}_{i+m} = f( y_{i}, y_{i-1}, ... y_{i-k+1} ) where \\hat{y}_{i+m} is a predicted value. In matrix forms we will define this model as: \\hat{Y} = f(X) where X = \\left( \\begin{array}{cccc} y_{k} & y_{k-1} & \\ldots & y_{1}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ y_{k+j} & y_{k+j-1} & \\ldots & y_{j+1}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\end{array} \\right) Y = \\left( \\begin{array}{c} y_{k+m} \\\\ \\vdots\\\\ y_{k+j+m}\\\\ \\vdots \\end{array} \\right) K = 20 M = 50 def AR_matrices ( Y , K , M ): X_AR = [] Y_AR = [] for i in range ( len ( Y )): if i < K - 1 : continue if i + M >= len ( Y ): break ax_ar = Y [ i + 1 - K : i + 1 ] . reshape ( - 1 , ) X_AR . append ( ax_ar ) ay_ar = Y [ i + M ] #[0] Y_AR . append ( ay_ar ) return np . array ( X_AR ), np . array ( Y_AR ) # prepare X and Y matrices X_AR , Y_AR = AR_matrices ( Y , K , M ) X_AR [: 2 ] array([[-0.94727033, -0.8491947 , -0.93092439, -0.94727033, -0.62035158, -0.93092439, -0.94727033, -0.60400564, -0.93092439, -0.66938939, -0.91457845, -0.60400564, -0.81650283, -0.89823251, -0.88188658, -0.93092439, -0.91457845, -0.78381095, -0.91457845, -0.93092439], [-0.8491947 , -0.93092439, -0.94727033, -0.62035158, -0.93092439, -0.94727033, -0.60400564, -0.93092439, -0.66938939, -0.91457845, -0.60400564, -0.81650283, -0.89823251, -0.88188658, -0.93092439, -0.91457845, -0.78381095, -0.91457845, -0.93092439, -0.80015689]]) Y_AR [: 2 ] array([[-0.66938939], [-0.96361626]]) Train / Test Split N = 300 X_AR_train , X_AR_test = X_AR [: N ], X_AR [ N :] Y_AR_train , Y_AR_test = Y_AR [: N ], Y_AR [ N :] Conditional GAN Link: https://medium.com/@ma.bagheri/a-tutorial-on-conditional-generative-adversarial-nets-keras-implementation-694dcafa6282 Conditional BiGAN On this case we will use: - X - condition for Generator G, Encoder E and Discriminator D; - Y - values we want to generate with the Generator G. import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import TensorDataset , DataLoader from torch.autograd import Variable DEVICE = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) class Generator ( nn . Module ): def __init__ ( self , n_inputs , n_outputs ): super ( Generator , self ) . __init__ () self . fc1 = nn . Linear ( n_inputs , 100 ) self . act1 = nn . ReLU () self . fc2 = nn . Linear ( 100 , 100 ) self . act2 = nn . ReLU () self . fc_out = nn . Linear ( 100 , n_outputs ) def forward ( self , cond , z ): x = torch . cat (( cond , z ), dim = 1 ) x = self . fc1 ( x ) x = self . act1 ( x ) x = self . fc2 ( x ) x = self . act2 ( x ) x = self . fc_out ( x ) return x class Encoder ( nn . Module ): def __init__ ( self , n_inputs , n_outputs ): super ( Encoder , self ) . __init__ () self . fc1 = nn . Linear ( n_inputs , 100 ) self . act1 = nn . ReLU () self . fc2 = nn . Linear ( 100 , 100 ) self . act2 = nn . ReLU () self . fc_out = nn . Linear ( 100 , n_outputs ) def forward ( self , cond , x ): x = torch . cat (( cond , x ), dim = 1 ) x = self . fc1 ( x ) x = self . act1 ( x ) x = self . fc2 ( x ) x = self . act2 ( x ) x = self . fc_out ( x ) return x class Discriminator ( nn . Module ): def __init__ ( self , n_inputs ): super ( Discriminator , self ) . __init__ () self . fc1 = nn . Linear ( n_inputs , 100 ) self . act1 = nn . ReLU () self . fc2 = nn . Linear ( 100 , 100 ) self . act2 = nn . ReLU () self . fc_out = nn . Linear ( 100 , 1 ) def forward ( self , cond , x , z ): xz = torch . cat (( cond , x , z ), dim = 1 ) x = self . fc1 ( xz ) x = self . act1 ( x ) x = self . fc2 ( x ) x = self . act2 ( x ) x = self . fc_out ( x ) return x class BiFitter ( object ): def __init__ ( self , generator , discriminator , encoder , batch_size = 32 , n_epochs = 10 , latent_dim = 1 , lr = 0.0001 , n_critic = 5 ): self . generator = generator self . discriminator = discriminator self . encoder = encoder self . batch_size = batch_size self . n_epochs = n_epochs self . latent_dim = latent_dim self . lr = lr self . n_critic = n_critic self . opt_gen = torch . optim . RMSprop ( list ( self . generator . parameters ()) + list ( self . encoder . parameters ()), lr = self . lr ) self . opt_disc = torch . optim . RMSprop ( self . discriminator . parameters (), lr = self . lr ) self . generator . to ( DEVICE ) self . discriminator . to ( DEVICE ) self . encoder . to ( DEVICE ) def fit ( self , X , Y ): Cond = torch . tensor ( X , dtype = torch . float , device = DEVICE ) Y_real = torch . tensor ( Y , dtype = torch . float , device = DEVICE ) dataset_real = TensorDataset ( Cond , Y_real ) # Turn on training self . generator . train ( True ) self . discriminator . train ( True ) self . encoder . train ( True ) self . loss_history = [] # Fit GAN for epoch in range ( self . n_epochs ): for i , ( cond_batch , real_batch ) in enumerate ( DataLoader ( dataset_real , batch_size = self . batch_size , shuffle = True )): # generate a batch of fake objects z_batch = Variable ( torch . tensor ( np . random . normal ( 0 , 1 , ( len ( real_batch ), self . latent_dim )), dtype = torch . float , device = DEVICE )) fake_batch = self . generator ( cond_batch , z_batch ) # encode real objects back into latent space z_enc_batch = self . encoder ( cond_batch , real_batch ) ### Discriminator real_output = self . discriminator ( cond_batch , real_batch , z_enc_batch ) fake_output = self . discriminator ( cond_batch , fake_batch , z_batch ) ### Loss loss_disc = - torch . mean ( real_output ) + torch . mean ( fake_output ) self . opt_disc . zero_grad () loss_disc . backward ( retain_graph = True ) self . opt_disc . step () # Clip weights of discriminator for p in discriminator . parameters (): p . data . clamp_ ( - 0.01 , 0.01 ) ### Train Generator if i % self . n_critic == 0 : real_output = self . discriminator ( cond_batch , real_batch , z_enc_batch ) fake_output = self . discriminator ( cond_batch , fake_batch , z_batch ) loss_gen = - torch . mean ( fake_output ) + torch . mean ( real_output ) self . opt_gen . zero_grad () loss_gen . backward () self . opt_gen . step () # caiculate and store loss after an epoch Z = Variable ( torch . tensor ( np . random . normal ( 0 , 1 , ( len ( Y_real ), self . latent_dim )), dtype = torch . float , device = DEVICE )) Y_fake = self . generator ( Cond , Z ) Z_enc = self . encoder ( Cond , Y_real ) real_output = self . discriminator ( Cond , Y_real , Z_enc ) fake_output = self . discriminator ( Cond , Y_fake , Z ) loss_epoch = torch . mean ( real_output ) - torch . mean ( fake_output ) self . loss_history . append ( loss_epoch . detach () . cpu ()) # Turn off training self . generator . train ( False ) self . discriminator . train ( False ) self . encoder . train ( False ) def generate ( self , N ): noise = Variable ( torch . tensor ( np . random . normal ( 0 , 1 , ( N , self . latent_dim )), dtype = torch . float , device = DEVICE )) # noise = Variable(torch.tensor(2*np.random.rand(N, self.latent_dim)-1, dtype=torch.float, device=DEVICE)) X_gen = self . generator ( noise ) return X_gen def discriminate ( self , X ): intput = torch . tensor ( X , dtype = torch . float , device = DEVICE ) output = self . discriminator ( intput ) . detach () . numpy () return output %% time latent_dim = 1 generator = Generator ( X_AR . shape [ 1 ] + latent_dim , 1 ) encoder = Encoder ( X_AR . shape [ 1 ] + 1 , latent_dim ) discriminator = Discriminator ( X_AR . shape [ 1 ] + latent_dim + 1 ) fitter = BiFitter ( generator , discriminator , encoder , batch_size = 50 , n_epochs = 5000 , latent_dim = latent_dim , lr = 0.0001 , n_critic = 5 ) fitter . fit ( X_AR_train , Y_AR_train ) CPU times: user 3min 16s, sys: 2.84 s, total: 3min 19s Wall time: 3min 21s # WGAN learning curve plt . figure ( figsize = ( 9 , 5 )) plt . plot ( fitter . loss_history ) plt . xlabel ( \"Epoch Number\" , size = 14 ) plt . ylabel ( \"Loss Function\" , size = 14 ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . title ( \"WGAN Learning Curve\" , size = 14 ) plt . grid ( b = 1 , linestyle = '--' , linewidth = 0.5 , color = '0.5' ) plt . show () Forecast Example # define conditions Cond_test = torch . tensor ( X_AR_test , dtype = torch . float , device = DEVICE ) Single Prediction # generate noise in latent space Z_test = torch . tensor ( np . random . normal ( 0 , 1 , ( len ( Y_AR_test ), latent_dim )), dtype = torch . float , device = DEVICE ) # make predictions using Generator G Y_pred_test = fitter . generator ( Cond_test , Z_test ) . cpu () . detach () . numpy () plt . figure ( figsize = ( 12 , 5 )) plt . plot ( Y_AR_test , label = 'True' , alpha = 1. ) plt . plot ( Y_pred_test , label = 'Prediction' , alpha = 1. ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . grid ( b = 1 ) #plt.xlim(300, 500) plt . show () Multiple Predictions predictions_test = [] for i in range ( 1000 ): # generate noise in latent space Z_test = torch . tensor ( np . random . normal ( 0 , 1 , ( len ( Y_AR_test ), latent_dim )), dtype = torch . float , device = DEVICE ) # make predictions using Generator G Y_pred_test = fitter . generator ( Cond_test , Z_test ) . cpu () . detach () . numpy () # store predictions predictions_test . append ( Y_pred_test [:, 0 ]) predictions_test = np . array ( predictions_test ) plt . figure ( figsize = ( 12 , 5 )) for Y_pred_test in predictions_test : plt . plot ( Y_pred_test , alpha = 0.01 , color = 'C0' ) plt . plot ( predictions_test . mean ( axis = 0 ), label = 'Prediction (mean)' , alpha = 1. , color = '0' ) plt . plot ( Y_AR_test , label = 'True' , alpha = 1. , color = 'C1' ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . grid ( b = 1 ) plt . xlim ( 0 , 700 ) #plt.ylim(0, 2) plt . show () Change-Point Detection based on Prediction Errors Link: https://towardsdatascience.com/anomaly-detection-time-series-4c661f6f165f Assume, that predictions \\hat{y}_{i} for each observation i are sampled form normal distribution \\mathcal{N}(\\mu_i, \\sigma_i) : where \\mu_i = \\frac{1}{n}\\sum_{k=1}^{n} \\hat{y}_{ik} \\sigma_i = \\sqrt{ \\frac{1}{n-1}\\sum_{k=1}^{n} (\\hat{y}_{ik} - \\mu_i)^2} Then, anomalies detection score can be estimated as following: Score_i = \\frac{|y_i - \\mu_i|}{\\sigma_i} where y_{i} is an observed value. This can be extrapolated to multidimensional cases as well. Link: https://link.springer.com/chapter/10.1007%2F978-1-4419-5525-8_3 # estimate mean of multiple predictions mean = predictions_test . mean ( axis = 0 ) # estimate standard deviation of multiple predictions sigma = predictions_test . std ( axis = 0 ) # calculate anomaly and change-point detection score score = np . abs ( Y_AR_test . reshape ( - 1 , ) - mean ) / sigma plt . figure ( figsize = ( 12 , 4 )) for Y_pred_test in predictions_test : plt . plot ( Y_pred_test , alpha = 0.01 , color = 'C0' ) plt . plot ( predictions_test . mean ( axis = 0 ), label = 'Prediction (mean)' , alpha = 1. , color = '0' ) plt . plot ( Y_AR_test , label = 'True' , alpha = 1. , color = 'C1' ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . grid ( b = 1 ) #plt.xlim(0, 500) #plt.ylim(0, 2) plt . show () plt . figure ( figsize = ( 12 , 4 )) plt . plot ( score , label = 'Anomaly & CP score' , alpha = 1. ) plt . plot ([ 1 ] * len ( score ), label = r '$1\\sigma$' , alpha = 1. ) plt . plot ([ 2 ] * len ( score ), label = r '$2\\sigma$' , alpha = 1. ) plt . plot ([ 3 ] * len ( score ), label = r '$3\\sigma$' , alpha = 1. ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . grid ( b = 1 ) #plt.xlim(0, 500) plt . ylim ( 0 , 10 ) plt . show () Alternative Approach Assume, that predictions \\hat{y}_{i} for each observation i are sampled form normal distribution \\mathcal{N}(\\mu_i, \\sigma_i) : p(\\hat{y}_{i}) = \\frac{1}{\\sqrt{2 \\pi} \\sigma_i} e^{ - \\frac{(\\hat{y}_{i}-\\mu_i)^2}{2\\sigma_i^2} } where \\mu_i = \\frac{1}{n}\\sum_{k=1}^{n} \\hat{y}_{ik} \\sigma_i = \\sqrt{ \\frac{1}{n-1}\\sum_{k=1}^{n} (\\hat{y}_{ik} - \\mu_i)^2} Then, cpange-point detection score is estimated as following: Score_i = \\frac{1}{w} \\sum_{k=i-w+1}^{i} - \\log p(y_{k}) where y_{k} is an observed value. This can be extrapolated to multidimensional cases as well. Link: https://dl.acm.org/doi/10.1145/775047.775148 def ln_p_normal ( x , mean , sigma ): ln_p = - 0.5 * np . log ( 2 * np . pi ) - np . log ( sigma ) - 0.5 * (( x - mean ) / sigma ) ** 2 return ln_p # calculate - log p(y) score = - ln_p_normal ( Y_AR_test . reshape ( - 1 , ), mean , sigma ) # calculate rolling average w = 50 score = np . convolve ( score , np . ones ( w ), 'same' ) / w plt . figure ( figsize = ( 12 , 4 )) for Y_pred_test in predictions_test : plt . plot ( Y_pred_test , alpha = 0.01 , color = 'C0' ) plt . plot ( predictions_test . mean ( axis = 0 ), label = 'Prediction (mean)' , alpha = 1. , color = '0' ) plt . plot ( Y_AR_test , label = 'True' , alpha = 1. , color = 'C1' ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . grid ( b = 1 ) #plt.xlim(0, 500) #plt.ylim(0, 2) plt . show () plt . figure ( figsize = ( 12 , 4 )) plt . plot ( score , label = 'Anomaly & CP score' , alpha = 1. ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . grid ( b = 1 ) #plt.xlim(0, 500) plt . ylim ( score . min (), 20 ) plt . show () Change-Point Detection based on BiGAN Reconstruction Errors BiGAN helps to detect anomalies by estimating the following score function A(x) : A(x) = \\alpha L_{G}(x) + (1 - \\alpha)L_{D}(x), where L_{G}(x) = || x - G(E(x)) ||_{1}, L_{D}(x) = || D(x, E(x)) - D(G(E(x)), E(x)) ||_{1}, \\alpha is an adjustable parameter. Cond_test = torch . tensor ( X_AR_test , dtype = torch . float , device = DEVICE ) Y_real_test = torch . tensor ( Y_AR_test , dtype = torch . float , device = DEVICE ) Z_enc_test = fitter . encoder ( Cond_test , Y_real_test ) Y_enc_pred_test = fitter . generator ( Cond_test , Z_enc_test ) real_output = fitter . discriminator ( Cond_test , Y_real_test , Z_enc_test ) . cpu () . detach () . numpy () pred_output = fitter . discriminator ( Cond_test , Y_enc_pred_test , Z_enc_test ) . cpu () . detach () . numpy () Y_enc_pred_test = Y_enc_pred_test . cpu () . detach () . numpy () Z_enc_test = Z_enc_test . cpu () . detach () . numpy () # calculate anomaly score A(x) alpha = 0.9 L_G = np . abs ( Y_AR_test - Y_enc_pred_test ) . sum ( axis = 1 ) L_D = np . abs ( real_output - pred_output ) . sum ( axis = 1 ) A = alpha * L_G + ( 1 - alpha ) * L_D plt . figure ( figsize = ( 12 , 4 )) for Y_pred_test in predictions_test : plt . plot ( Y_pred_test , alpha = 0.01 , color = 'C0' ) plt . plot ( predictions_test . mean ( axis = 0 ), label = 'Prediction (mean)' , alpha = 1. , color = '0' ) plt . plot ( Y_AR_test , label = 'True' , alpha = 1. , color = 'C1' ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . grid ( b = 1 ) #plt.xlim(0, 500) #plt.ylim(0, 2) plt . show () plt . figure ( figsize = ( 12 , 4 )) plt . plot ( A , label = 'Anomaly score' , alpha = 1. ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . grid ( b = 1 ) #plt.xlim(0, 500) plt . ylim ( 0 , 5 ) plt . show () Resources Roerich: https://github.com/HSE-LAMBDA/roerich Ruptures: https://github.com/deepcharles/ruptures TIRE: https://github.com/HolyBayes/TIRE_pytorch KL CPD: https://github.com/HolyBayes/klcpd","title":"2 time series gan"},{"location":"examples/2-time-series-gan/#3-2021","text":"","title":" 3 \u0444\u0435\u0432\u0440\u0430\u043b\u044f 2021 \u0433.  \u041c\u043e\u0441\u043a\u0432\u0430"},{"location":"examples/2-time-series-gan/#_1","text":"","title":"\u0422\u0443\u0442\u043e\u0440\u0438\u0430\u043b"},{"location":"examples/2-time-series-gan/#_2","text":"","title":"\u041f\u043e \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u044e \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0439 \u0438 \u0442\u043e\u0447\u0435\u043a \u0440\u0430\u0437\u043b\u0430\u0434\u043a\u0438 \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u044b\u043c\u0438 \u043c\u043e\u0434\u0435\u043b\u044f\u043c\u0438"},{"location":"examples/2-time-series-gan/#content","text":"Change-points and Anomalies Conditional GAN Conditional BiGAN Change-point and Anomalies Detection Based on Prediction Errors Change-point and Anomalies Detection Based on BiGAN Reconstruction Errors Additional Resources","title":"Content"},{"location":"examples/2-time-series-gan/#anomalies-vs-change-points","text":"% matplotlib inline import matplotlib.pyplot as plt import numpy as np import pandas as pd","title":"Anomalies vs Change-Points"},{"location":"examples/2-time-series-gan/#data-reading","text":"Yahoo S5 - A Labeled Anomaly Detection Dataset, version 1.0(16M) Automatic anomaly detection is critical in today's world where the sheer volume of data makes it impossible to tag outliers manually. The goal of this dataset is to benchmark your anomaly detection algorithm. The dataset consists of real and synthetic time-series with tagged anomaly points. The dataset tests the detection accuracy of various anomaly-types including outliers and change-points. The synthetic dataset consists of time-series with varying trend, noise and seasonality. The real dataset consists of time-series representing the metrics of various Yahoo services. Link: https://webscope.sandbox.yahoo.com/catalog.php?datatype=s&did=70 #!wget https://raw.githubusercontent.com/HSE-LAMBDA/OpenTalks2021_tutorial/main/data/Yahoo_S5/A4Benchmark/A4Benchmark-TS12.csv #!wget https://raw.githubusercontent.com/HSE-LAMBDA/OpenTalks2021_tutorial/main/data/Yahoo_S5/A4Benchmark/A4Benchmark-TS37.csv #!wget https://raw.githubusercontent.com/HSE-LAMBDA/OpenTalks2021_tutorial/main/data/Yahoo_S5/A1Benchmark/real_57.csv ! wget https : // raw . githubusercontent . com / HSE - LAMBDA / OpenTalks2021_tutorial / main / data / Yahoo_S5 / A1Benchmark / real_61 . csv --2021-02-02 19:12:08-- https://raw.githubusercontent.com/HSE-LAMBDA/OpenTalks2021_tutorial/main/data/Yahoo_S5/A1Benchmark/real_61.csv Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133 Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 13774 (13K) [text/plain] Saving to: \u2018real_61.csv.2\u2019 real_61.csv.2 100%[===================>] 13.45K --.-KB/s in 0s 2021-02-02 19:12:08 (94.5 MB/s) - \u2018real_61.csv.2\u2019 saved [13774/13774] data = pd . read_csv ( \"real_61.csv\" , sep = ',' ) data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } timestamp value is_anomaly 0 1 2 0 1 2 14 0 2 3 4 0 3 4 2 0 4 5 42 0 # get a time series Y = data [[ 'value' ]] . values plt . figure ( figsize = ( 12 , 5 )) plt . plot ( Y , label = 'True' ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . grid ( b = 1 ) plt . show ()","title":"Data Reading"},{"location":"examples/2-time-series-gan/#preprocessing","text":"# scaling from sklearn.preprocessing import StandardScaler Y = StandardScaler () . fit_transform ( Y ) plt . figure ( figsize = ( 12 , 5 )) plt . plot ( Y , label = 'True' ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . grid ( b = 1 ) plt . show ()","title":"Preprocessing"},{"location":"examples/2-time-series-gan/#autoregressive-model-for-time-series-forecast","text":"Consider y_1, y_2, ..., y_i, ..., y_N are observations of a time series. Autoregressive Model for the time series forecast assumes the following: \\hat{y}_{i+m} = f( y_{i}, y_{i-1}, ... y_{i-k+1} ) where \\hat{y}_{i+m} is a predicted value. In matrix forms we will define this model as: \\hat{Y} = f(X) where X = \\left( \\begin{array}{cccc} y_{k} & y_{k-1} & \\ldots & y_{1}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ y_{k+j} & y_{k+j-1} & \\ldots & y_{j+1}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\end{array} \\right) Y = \\left( \\begin{array}{c} y_{k+m} \\\\ \\vdots\\\\ y_{k+j+m}\\\\ \\vdots \\end{array} \\right) K = 20 M = 50 def AR_matrices ( Y , K , M ): X_AR = [] Y_AR = [] for i in range ( len ( Y )): if i < K - 1 : continue if i + M >= len ( Y ): break ax_ar = Y [ i + 1 - K : i + 1 ] . reshape ( - 1 , ) X_AR . append ( ax_ar ) ay_ar = Y [ i + M ] #[0] Y_AR . append ( ay_ar ) return np . array ( X_AR ), np . array ( Y_AR ) # prepare X and Y matrices X_AR , Y_AR = AR_matrices ( Y , K , M ) X_AR [: 2 ] array([[-0.94727033, -0.8491947 , -0.93092439, -0.94727033, -0.62035158, -0.93092439, -0.94727033, -0.60400564, -0.93092439, -0.66938939, -0.91457845, -0.60400564, -0.81650283, -0.89823251, -0.88188658, -0.93092439, -0.91457845, -0.78381095, -0.91457845, -0.93092439], [-0.8491947 , -0.93092439, -0.94727033, -0.62035158, -0.93092439, -0.94727033, -0.60400564, -0.93092439, -0.66938939, -0.91457845, -0.60400564, -0.81650283, -0.89823251, -0.88188658, -0.93092439, -0.91457845, -0.78381095, -0.91457845, -0.93092439, -0.80015689]]) Y_AR [: 2 ] array([[-0.66938939], [-0.96361626]])","title":"Autoregressive Model for Time Series Forecast"},{"location":"examples/2-time-series-gan/#train-test-split","text":"N = 300 X_AR_train , X_AR_test = X_AR [: N ], X_AR [ N :] Y_AR_train , Y_AR_test = Y_AR [: N ], Y_AR [ N :]","title":"Train / Test Split"},{"location":"examples/2-time-series-gan/#conditional-gan","text":"Link: https://medium.com/@ma.bagheri/a-tutorial-on-conditional-generative-adversarial-nets-keras-implementation-694dcafa6282","title":"Conditional GAN"},{"location":"examples/2-time-series-gan/#conditional-bigan","text":"On this case we will use: - X - condition for Generator G, Encoder E and Discriminator D; - Y - values we want to generate with the Generator G. import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import TensorDataset , DataLoader from torch.autograd import Variable DEVICE = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) class Generator ( nn . Module ): def __init__ ( self , n_inputs , n_outputs ): super ( Generator , self ) . __init__ () self . fc1 = nn . Linear ( n_inputs , 100 ) self . act1 = nn . ReLU () self . fc2 = nn . Linear ( 100 , 100 ) self . act2 = nn . ReLU () self . fc_out = nn . Linear ( 100 , n_outputs ) def forward ( self , cond , z ): x = torch . cat (( cond , z ), dim = 1 ) x = self . fc1 ( x ) x = self . act1 ( x ) x = self . fc2 ( x ) x = self . act2 ( x ) x = self . fc_out ( x ) return x class Encoder ( nn . Module ): def __init__ ( self , n_inputs , n_outputs ): super ( Encoder , self ) . __init__ () self . fc1 = nn . Linear ( n_inputs , 100 ) self . act1 = nn . ReLU () self . fc2 = nn . Linear ( 100 , 100 ) self . act2 = nn . ReLU () self . fc_out = nn . Linear ( 100 , n_outputs ) def forward ( self , cond , x ): x = torch . cat (( cond , x ), dim = 1 ) x = self . fc1 ( x ) x = self . act1 ( x ) x = self . fc2 ( x ) x = self . act2 ( x ) x = self . fc_out ( x ) return x class Discriminator ( nn . Module ): def __init__ ( self , n_inputs ): super ( Discriminator , self ) . __init__ () self . fc1 = nn . Linear ( n_inputs , 100 ) self . act1 = nn . ReLU () self . fc2 = nn . Linear ( 100 , 100 ) self . act2 = nn . ReLU () self . fc_out = nn . Linear ( 100 , 1 ) def forward ( self , cond , x , z ): xz = torch . cat (( cond , x , z ), dim = 1 ) x = self . fc1 ( xz ) x = self . act1 ( x ) x = self . fc2 ( x ) x = self . act2 ( x ) x = self . fc_out ( x ) return x class BiFitter ( object ): def __init__ ( self , generator , discriminator , encoder , batch_size = 32 , n_epochs = 10 , latent_dim = 1 , lr = 0.0001 , n_critic = 5 ): self . generator = generator self . discriminator = discriminator self . encoder = encoder self . batch_size = batch_size self . n_epochs = n_epochs self . latent_dim = latent_dim self . lr = lr self . n_critic = n_critic self . opt_gen = torch . optim . RMSprop ( list ( self . generator . parameters ()) + list ( self . encoder . parameters ()), lr = self . lr ) self . opt_disc = torch . optim . RMSprop ( self . discriminator . parameters (), lr = self . lr ) self . generator . to ( DEVICE ) self . discriminator . to ( DEVICE ) self . encoder . to ( DEVICE ) def fit ( self , X , Y ): Cond = torch . tensor ( X , dtype = torch . float , device = DEVICE ) Y_real = torch . tensor ( Y , dtype = torch . float , device = DEVICE ) dataset_real = TensorDataset ( Cond , Y_real ) # Turn on training self . generator . train ( True ) self . discriminator . train ( True ) self . encoder . train ( True ) self . loss_history = [] # Fit GAN for epoch in range ( self . n_epochs ): for i , ( cond_batch , real_batch ) in enumerate ( DataLoader ( dataset_real , batch_size = self . batch_size , shuffle = True )): # generate a batch of fake objects z_batch = Variable ( torch . tensor ( np . random . normal ( 0 , 1 , ( len ( real_batch ), self . latent_dim )), dtype = torch . float , device = DEVICE )) fake_batch = self . generator ( cond_batch , z_batch ) # encode real objects back into latent space z_enc_batch = self . encoder ( cond_batch , real_batch ) ### Discriminator real_output = self . discriminator ( cond_batch , real_batch , z_enc_batch ) fake_output = self . discriminator ( cond_batch , fake_batch , z_batch ) ### Loss loss_disc = - torch . mean ( real_output ) + torch . mean ( fake_output ) self . opt_disc . zero_grad () loss_disc . backward ( retain_graph = True ) self . opt_disc . step () # Clip weights of discriminator for p in discriminator . parameters (): p . data . clamp_ ( - 0.01 , 0.01 ) ### Train Generator if i % self . n_critic == 0 : real_output = self . discriminator ( cond_batch , real_batch , z_enc_batch ) fake_output = self . discriminator ( cond_batch , fake_batch , z_batch ) loss_gen = - torch . mean ( fake_output ) + torch . mean ( real_output ) self . opt_gen . zero_grad () loss_gen . backward () self . opt_gen . step () # caiculate and store loss after an epoch Z = Variable ( torch . tensor ( np . random . normal ( 0 , 1 , ( len ( Y_real ), self . latent_dim )), dtype = torch . float , device = DEVICE )) Y_fake = self . generator ( Cond , Z ) Z_enc = self . encoder ( Cond , Y_real ) real_output = self . discriminator ( Cond , Y_real , Z_enc ) fake_output = self . discriminator ( Cond , Y_fake , Z ) loss_epoch = torch . mean ( real_output ) - torch . mean ( fake_output ) self . loss_history . append ( loss_epoch . detach () . cpu ()) # Turn off training self . generator . train ( False ) self . discriminator . train ( False ) self . encoder . train ( False ) def generate ( self , N ): noise = Variable ( torch . tensor ( np . random . normal ( 0 , 1 , ( N , self . latent_dim )), dtype = torch . float , device = DEVICE )) # noise = Variable(torch.tensor(2*np.random.rand(N, self.latent_dim)-1, dtype=torch.float, device=DEVICE)) X_gen = self . generator ( noise ) return X_gen def discriminate ( self , X ): intput = torch . tensor ( X , dtype = torch . float , device = DEVICE ) output = self . discriminator ( intput ) . detach () . numpy () return output %% time latent_dim = 1 generator = Generator ( X_AR . shape [ 1 ] + latent_dim , 1 ) encoder = Encoder ( X_AR . shape [ 1 ] + 1 , latent_dim ) discriminator = Discriminator ( X_AR . shape [ 1 ] + latent_dim + 1 ) fitter = BiFitter ( generator , discriminator , encoder , batch_size = 50 , n_epochs = 5000 , latent_dim = latent_dim , lr = 0.0001 , n_critic = 5 ) fitter . fit ( X_AR_train , Y_AR_train ) CPU times: user 3min 16s, sys: 2.84 s, total: 3min 19s Wall time: 3min 21s # WGAN learning curve plt . figure ( figsize = ( 9 , 5 )) plt . plot ( fitter . loss_history ) plt . xlabel ( \"Epoch Number\" , size = 14 ) plt . ylabel ( \"Loss Function\" , size = 14 ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . title ( \"WGAN Learning Curve\" , size = 14 ) plt . grid ( b = 1 , linestyle = '--' , linewidth = 0.5 , color = '0.5' ) plt . show ()","title":"Conditional BiGAN"},{"location":"examples/2-time-series-gan/#forecast-example","text":"# define conditions Cond_test = torch . tensor ( X_AR_test , dtype = torch . float , device = DEVICE )","title":"Forecast Example"},{"location":"examples/2-time-series-gan/#single-prediction","text":"# generate noise in latent space Z_test = torch . tensor ( np . random . normal ( 0 , 1 , ( len ( Y_AR_test ), latent_dim )), dtype = torch . float , device = DEVICE ) # make predictions using Generator G Y_pred_test = fitter . generator ( Cond_test , Z_test ) . cpu () . detach () . numpy () plt . figure ( figsize = ( 12 , 5 )) plt . plot ( Y_AR_test , label = 'True' , alpha = 1. ) plt . plot ( Y_pred_test , label = 'Prediction' , alpha = 1. ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . grid ( b = 1 ) #plt.xlim(300, 500) plt . show ()","title":"Single Prediction"},{"location":"examples/2-time-series-gan/#multiple-predictions","text":"predictions_test = [] for i in range ( 1000 ): # generate noise in latent space Z_test = torch . tensor ( np . random . normal ( 0 , 1 , ( len ( Y_AR_test ), latent_dim )), dtype = torch . float , device = DEVICE ) # make predictions using Generator G Y_pred_test = fitter . generator ( Cond_test , Z_test ) . cpu () . detach () . numpy () # store predictions predictions_test . append ( Y_pred_test [:, 0 ]) predictions_test = np . array ( predictions_test ) plt . figure ( figsize = ( 12 , 5 )) for Y_pred_test in predictions_test : plt . plot ( Y_pred_test , alpha = 0.01 , color = 'C0' ) plt . plot ( predictions_test . mean ( axis = 0 ), label = 'Prediction (mean)' , alpha = 1. , color = '0' ) plt . plot ( Y_AR_test , label = 'True' , alpha = 1. , color = 'C1' ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . grid ( b = 1 ) plt . xlim ( 0 , 700 ) #plt.ylim(0, 2) plt . show ()","title":"Multiple Predictions"},{"location":"examples/2-time-series-gan/#change-point-detection-based-on-prediction-errors","text":"Link: https://towardsdatascience.com/anomaly-detection-time-series-4c661f6f165f Assume, that predictions \\hat{y}_{i} for each observation i are sampled form normal distribution \\mathcal{N}(\\mu_i, \\sigma_i) : where \\mu_i = \\frac{1}{n}\\sum_{k=1}^{n} \\hat{y}_{ik} \\sigma_i = \\sqrt{ \\frac{1}{n-1}\\sum_{k=1}^{n} (\\hat{y}_{ik} - \\mu_i)^2} Then, anomalies detection score can be estimated as following: Score_i = \\frac{|y_i - \\mu_i|}{\\sigma_i} where y_{i} is an observed value. This can be extrapolated to multidimensional cases as well. Link: https://link.springer.com/chapter/10.1007%2F978-1-4419-5525-8_3 # estimate mean of multiple predictions mean = predictions_test . mean ( axis = 0 ) # estimate standard deviation of multiple predictions sigma = predictions_test . std ( axis = 0 ) # calculate anomaly and change-point detection score score = np . abs ( Y_AR_test . reshape ( - 1 , ) - mean ) / sigma plt . figure ( figsize = ( 12 , 4 )) for Y_pred_test in predictions_test : plt . plot ( Y_pred_test , alpha = 0.01 , color = 'C0' ) plt . plot ( predictions_test . mean ( axis = 0 ), label = 'Prediction (mean)' , alpha = 1. , color = '0' ) plt . plot ( Y_AR_test , label = 'True' , alpha = 1. , color = 'C1' ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . grid ( b = 1 ) #plt.xlim(0, 500) #plt.ylim(0, 2) plt . show () plt . figure ( figsize = ( 12 , 4 )) plt . plot ( score , label = 'Anomaly & CP score' , alpha = 1. ) plt . plot ([ 1 ] * len ( score ), label = r '$1\\sigma$' , alpha = 1. ) plt . plot ([ 2 ] * len ( score ), label = r '$2\\sigma$' , alpha = 1. ) plt . plot ([ 3 ] * len ( score ), label = r '$3\\sigma$' , alpha = 1. ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . grid ( b = 1 ) #plt.xlim(0, 500) plt . ylim ( 0 , 10 ) plt . show ()","title":"Change-Point Detection based on Prediction Errors"},{"location":"examples/2-time-series-gan/#alternative-approach","text":"Assume, that predictions \\hat{y}_{i} for each observation i are sampled form normal distribution \\mathcal{N}(\\mu_i, \\sigma_i) : p(\\hat{y}_{i}) = \\frac{1}{\\sqrt{2 \\pi} \\sigma_i} e^{ - \\frac{(\\hat{y}_{i}-\\mu_i)^2}{2\\sigma_i^2} } where \\mu_i = \\frac{1}{n}\\sum_{k=1}^{n} \\hat{y}_{ik} \\sigma_i = \\sqrt{ \\frac{1}{n-1}\\sum_{k=1}^{n} (\\hat{y}_{ik} - \\mu_i)^2} Then, cpange-point detection score is estimated as following: Score_i = \\frac{1}{w} \\sum_{k=i-w+1}^{i} - \\log p(y_{k}) where y_{k} is an observed value. This can be extrapolated to multidimensional cases as well. Link: https://dl.acm.org/doi/10.1145/775047.775148 def ln_p_normal ( x , mean , sigma ): ln_p = - 0.5 * np . log ( 2 * np . pi ) - np . log ( sigma ) - 0.5 * (( x - mean ) / sigma ) ** 2 return ln_p # calculate - log p(y) score = - ln_p_normal ( Y_AR_test . reshape ( - 1 , ), mean , sigma ) # calculate rolling average w = 50 score = np . convolve ( score , np . ones ( w ), 'same' ) / w plt . figure ( figsize = ( 12 , 4 )) for Y_pred_test in predictions_test : plt . plot ( Y_pred_test , alpha = 0.01 , color = 'C0' ) plt . plot ( predictions_test . mean ( axis = 0 ), label = 'Prediction (mean)' , alpha = 1. , color = '0' ) plt . plot ( Y_AR_test , label = 'True' , alpha = 1. , color = 'C1' ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . grid ( b = 1 ) #plt.xlim(0, 500) #plt.ylim(0, 2) plt . show () plt . figure ( figsize = ( 12 , 4 )) plt . plot ( score , label = 'Anomaly & CP score' , alpha = 1. ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . grid ( b = 1 ) #plt.xlim(0, 500) plt . ylim ( score . min (), 20 ) plt . show ()","title":"Alternative Approach"},{"location":"examples/2-time-series-gan/#change-point-detection-based-on-bigan-reconstruction-errors","text":"BiGAN helps to detect anomalies by estimating the following score function A(x) : A(x) = \\alpha L_{G}(x) + (1 - \\alpha)L_{D}(x), where L_{G}(x) = || x - G(E(x)) ||_{1}, L_{D}(x) = || D(x, E(x)) - D(G(E(x)), E(x)) ||_{1}, \\alpha is an adjustable parameter. Cond_test = torch . tensor ( X_AR_test , dtype = torch . float , device = DEVICE ) Y_real_test = torch . tensor ( Y_AR_test , dtype = torch . float , device = DEVICE ) Z_enc_test = fitter . encoder ( Cond_test , Y_real_test ) Y_enc_pred_test = fitter . generator ( Cond_test , Z_enc_test ) real_output = fitter . discriminator ( Cond_test , Y_real_test , Z_enc_test ) . cpu () . detach () . numpy () pred_output = fitter . discriminator ( Cond_test , Y_enc_pred_test , Z_enc_test ) . cpu () . detach () . numpy () Y_enc_pred_test = Y_enc_pred_test . cpu () . detach () . numpy () Z_enc_test = Z_enc_test . cpu () . detach () . numpy () # calculate anomaly score A(x) alpha = 0.9 L_G = np . abs ( Y_AR_test - Y_enc_pred_test ) . sum ( axis = 1 ) L_D = np . abs ( real_output - pred_output ) . sum ( axis = 1 ) A = alpha * L_G + ( 1 - alpha ) * L_D plt . figure ( figsize = ( 12 , 4 )) for Y_pred_test in predictions_test : plt . plot ( Y_pred_test , alpha = 0.01 , color = 'C0' ) plt . plot ( predictions_test . mean ( axis = 0 ), label = 'Prediction (mean)' , alpha = 1. , color = '0' ) plt . plot ( Y_AR_test , label = 'True' , alpha = 1. , color = 'C1' ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . grid ( b = 1 ) #plt.xlim(0, 500) #plt.ylim(0, 2) plt . show () plt . figure ( figsize = ( 12 , 4 )) plt . plot ( A , label = 'Anomaly score' , alpha = 1. ) plt . xticks ( size = 14 ) plt . yticks ( size = 14 ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . grid ( b = 1 ) #plt.xlim(0, 500) plt . ylim ( 0 , 5 ) plt . show ()","title":"Change-Point Detection based on BiGAN Reconstruction Errors"},{"location":"examples/2-time-series-gan/#resources","text":"Roerich: https://github.com/HSE-LAMBDA/roerich Ruptures: https://github.com/deepcharles/ruptures TIRE: https://github.com/HolyBayes/TIRE_pytorch KL CPD: https://github.com/HolyBayes/klcpd","title":"Resources"},{"location":"examples/Untitled/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); # import of basic libraries import matplotlib.pyplot as plt from matplotlib import cm import pandas as pd import numpy as np import probaforms from probaforms.metrics import frechet_distance , maximum_mean_discrepancy , kolmogorov_smirnov_1d , cramer_von_mises_1d , roc_auc_score_1d , anderson_darling_1d from probaforms.metrics import kullback_leibler_1d , jensen_shannon_1d from probaforms.metrics import kullback_leibler_1d_kde , jensen_shannon_1d_kde N = 1000 X_real = np . random . multivariate_normal ([ 0 , 0 ], [[ 1 , 0.7 ], [ 0.7 , 1 ]], N ) X_fake = np . random . multivariate_normal ([ 0 , 0.1 ], [[ 1 , 0.7 ], [ 0.7 , 1 ]], N ) plt . scatter ( X_real [:, 0 ], X_real [:, 1 ]) plt . scatter ( X_fake [:, 0 ], X_fake [:, 1 ]) plt . show () # X_real = np.random.uniform(0, 2, (N, 2))#.reshape(-1, 1) # X_fake = np.random.uniform(2, 4, (N, 2))#.reshape(-1, 1) # X_real = np.random.normal(0, 1, 1*N).reshape(-1, 1) # X_fake = np.random.normal(1, 1, 10*N).reshape(-1, 1) # plt.hist(X_real, bins=100) # plt.hist(X_fake, bins=100) # plt.show() frechet_distance ( X_real , X_fake ) (0.013181966971982057, 0.007189378587489877) maximum_mean_discrepancy ( X_real , X_fake ) (0.0018034432785270017, 0.0009774633351036257) %% time kolmogorov_smirnov_1d ( X_real , X_fake ) CPU times: user 156 ms, sys: 349 ms, total: 505 ms Wall time: 94.7 ms (0.05169500000000001, 0.010144923607400895) %% time cramer_von_mises_1d ( X_real , X_fake ) CPU times: user 117 ms, sys: 257 ms, total: 373 ms Wall time: 73.4 ms (0.3036530349999811, 0.17356490141291958) %% time anderson_darling_1d ( X_real , X_fake ) CPU times: user 170 ms, sys: 267 ms, total: 436 ms Wall time: 121 ms (0.8314070682204191, 0.9449794727511458) %% time roc_auc_score_1d ( X_real , X_fake ) CPU times: user 184 ms, sys: 1.53 ms, total: 185 ms Wall time: 185 ms (0.51486465, 0.006217353362766193) %% time kullback_leibler_1d ( X_real , X_fake , bins = 10 ) CPU times: user 56.6 ms, sys: 936 \u00b5s, total: 57.6 ms Wall time: 58.3 ms (0.031703702395971936, 0.01280640090355048) jensen_shannon_1d ( X_real , X_fake , bins = 10 ) (0.0044684660882367755, 0.0012118941991319965) %% time kullback_leibler_1d_kde ( X_real , X_fake ) CPU times: user 2.23 s, sys: 10.4 ms, total: 2.24 s Wall time: 2.24 s (0.011177978627755237, 0.004562473333272194) jensen_shannon_1d_kde ( X_real , X_fake ) (0.0026475359706036273, 0.0007658134445861451) mus = [] sis = [] bins = np . arange ( 5 , 100 , 1 ) for b in bins : mu , si = kullback_leibler_1d ( X_real , X_fake , bins = b ) mus . append ( mu ) sis . append ( si ) mus = np . array ( mus ) sis = np . array ( sis ) plt . plot ( bins , mus ) plt . show () mus = [] bins = np . arange ( 5 , 100 , 1 ) for b in bins : mu , _ = jensen_shannon_1d ( X_real , X_fake , bins = b ) mus . append ( mu ) plt . plot ( bins , mus ) plt . show () np . log ( 2 ) 0.6931471805599453 from sklearn.neighbors import KernelDensity %% time kd = KernelDensity ( bandwidth = 'silverman' ) x = X_fake [:, [ 0 ]] #x = np.concatenate((X_fake[:, [0]], X_real[:, [0]])) kd . fit ( x ) CPU times: user 1.02 ms, sys: 30 \u00b5s, total: 1.05 ms Wall time: 1.03 ms #sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\u25b8\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\u25be\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;} KernelDensity(bandwidth='silverman') In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. KernelDensity KernelDensity(bandwidth='silverman') %% time #a = np.linspace(x.min(), x.max(), 10).reshape(-1, 1) a = np . linspace ( - 5 , 10 , 101 ) . reshape ( - 1 , 1 ) p = np . exp ( kd . score_samples ( a )) p /= p . sum () CPU times: user 14 ms, sys: 251 \u00b5s, total: 14.2 ms Wall time: 14.1 ms p array([6.65811824e-154, 1.60520146e-147, 2.81763138e-141, 3.60101508e-135, 3.35090304e-129, 2.27041615e-123, 1.12012587e-117, 4.02399644e-112, 1.05266463e-106, 2.00528970e-101, 2.78184272e-096, 2.81041798e-091, 2.06779557e-086, 1.10804932e-081, 4.32457071e-077, 1.22935762e-072, 2.54557083e-068, 3.83959957e-064, 4.21895158e-060, 3.37728230e-056, 1.96971531e-052, 8.37037811e-049, 2.59195977e-045, 5.84916114e-042, 9.62026697e-039, 1.15335114e-035, 1.00803963e-032, 6.42400547e-030, 2.98558329e-027, 1.01215088e-024, 2.50363253e-022, 4.52006908e-020, 5.95851604e-018, 5.73794657e-016, 4.03882275e-014, 2.07945267e-012, 7.83853833e-011, 2.16577485e-009, 4.39260698e-008, 6.55217874e-007, 7.20571492e-006, 5.86155153e-005, 3.54229492e-004, 1.59977557e-003, 5.44341865e-003, 1.41152811e-002, 2.83550890e-002, 4.51926573e-002, 5.91704903e-002, 6.68062719e-002, 6.90485380e-002, 6.91611300e-002, 6.94925332e-002, 7.06206953e-002, 7.23650267e-002, 7.44339713e-002, 7.59521937e-002, 7.49640275e-002, 6.89836301e-002, 5.66465557e-002, 3.96348352e-002, 2.26523092e-002, 1.02255102e-002, 3.55893478e-003, 9.39547990e-004, 1.86106225e-004, 2.74588310e-005, 3.00270169e-006, 2.42502551e-007, 1.44268939e-008, 6.31010424e-010, 2.02606097e-011, 4.76981008e-013, 8.22548464e-015, 1.03821507e-016, 9.58494143e-019, 6.46880169e-021, 3.18994000e-023, 1.14891249e-025, 3.02122337e-028, 5.79877219e-031, 8.12137279e-034, 8.29774851e-037, 6.18355183e-040, 3.36033556e-043, 1.33144497e-046, 3.84588736e-050, 8.09741267e-054, 1.24257283e-057, 1.38956205e-061, 1.13233173e-065, 6.72313974e-070, 2.90830457e-074, 9.16529701e-079, 2.10409062e-083, 3.51858714e-088, 4.28583083e-093, 3.80227348e-098, 2.45682237e-103, 1.15613417e-108, 3.96214299e-114]) plt . plot ( a , p ) plt . show () plt . hist ( x , bins = 100 ) plt . show () from probaforms.metrics import maximum_mean_discrepancy from probaforms.models.interfaces import GenModel def subclasses ( cls ): return set ( cls . __subclasses__ ()) . union ( s for c in cls . __subclasses__ () for s in subclasses ( c )) subclasses ( GenModel ) {probaforms.models.realnvp.RealNVP} from probaforms import metrics dir ( metrics ) for f in metrics . __all__ : f . --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In[223], line 5 3 dir (metrics) 4 for f in metrics . __all__: ----> 5 f ( ) TypeError : 'str' object is not callable from inspect import getmembers , isfunction for f in getmembers ( metrics , isfunction ): print ( f [ 1 ]( X_real , X_fake )) (0.1551333509553631, 0.08726909764210516) (0.055396212963567296, 0.014707551332771785) (0.1542, 0.0269325082381868) (0.5726464268961485, 0.25541443717036655) (0.01849286009372232, 0.009703837639500987) funcs = [ i [ 1 ] for i in getmembers ( metrics , isfunction )] funcs [<function probaforms.metrics.fd.frechet_distance(X_real, X_fake, n_iters=100, standardize=False)>, <function probaforms.metrics.div1d.jensen_shannon_1d(X_real, X_fake, n_iters=100, bins=10)>, <function probaforms.metrics.ks1d.kolmogorov_smirnov_1d(X_real, X_fake, n_iters=100)>, <function probaforms.metrics.div1d.kullback_leibler_1d(X_real, X_fake, n_iters=100, bins=10)>, <function probaforms.metrics.mmd.maximum_mean_discrepancy(X, Y, n_iters=100, standardize=True)>] np . histogram ( X_real ) (array([ 3, 20, 105, 271, 498, 559, 381, 126, 28, 9]), array([-3.71608916, -2.99241292, -2.26873667, -1.54506042, -0.82138417, -0.09770792, 0.62596833, 1.34964458, 2.07332083, 2.79699708, 3.52067333])) def compute_probs1d ( data , bins = 10 ): h , e = np . histogram ( data , bins ) p = h / h . sum () return p , e def get_probs1d ( data1 , data2 , bins = 10 ): data12 = np . concatenate (( data1 , data2 ), axis = 0 ) _ , e = compute_probs1d ( data12 , bins ) p , _ = compute_probs1d ( data1 , e ) q , _ = compute_probs1d ( data2 , e ) return p , q def kl_divergence ( p , q ): return np . sum ( p * np . log ( p / q )) p , q = get_probs1d ( X_real , X_fake , 10 ) p , q (array([0.0015, 0.0215, 0.0915, 0.1955, 0.3 , 0.2325, 0.1245, 0.0295, 0.0035, 0. ]), array([0.0005, 0.01 , 0.0555, 0.1275, 0.2215, 0.265 , 0.187 , 0.0935, 0.0345, 0.005 ])) e = 10 **- 10 kl_divergence ( p + e , q + e ) 0.11531850969046133 p / q array([3. , 2.15 , 1.64864865, 1.53333333, 1.35440181, 0.87735849, 0.6657754 , 0.31550802, 0.10144928, 0. ]) p . sum () 1.0 for i in zip ([ 1 ], [ 3 ]): print ( i ) (1, 3) lambda x : ( x [ 0 ] != 0 ) & ( x [ 1 ] != 0 ), zip ([ 1 ], [ 3 ]) (<function __main__.<lambda>(x)>, <zip at 0x7fc0f817d8c0>) def support_intersection ( p , q ): sup_int = ( list ( filter ( lambda x : ( x [ 0 ] != 0 ) & ( x [ 1 ] != 0 ), zip ( p , q ) ) ) ) return sup_int support_intersection ([ 1 , 5 ], [ 3 , 3 ]) [(1, 3), (5, 3)] def a ( x ): return x a ( 1 ) 1 def b ( a , z , * args ): return a ( z , * args ) b ( a , 3 ) 3 0.1 * np . log ( 0.1 / 10 **- 100 ) 22.795592420641054 from sklearn.model_selection import KFold kf = KFold ( n_splits = 5 , shuffle = True ) for (( _ , test_ids1 ), ( _ , test_ids2 )) in zip ( kf . split ( X_real ), kf . split ( X_fake )): print ( test_ids1 . shape , test_ids2 . shape ) (20,) (200,) (20,) (200,) (20,) (200,) (20,) (200,) (20,) (200,) test_ids1 array([ 1, 2, 19, 24, 25, 29, 31, 37, 38, 41, 44, 46, 55, 56, 59, 72, 75, 76, 83, 92]) test_ids2 array([ 0, 1, 7, 9, 10, 11, 20, 24, 33, 35, 43, 44, 47, 48, 50, 53, 54, 55, 61, 65, 66, 73, 81, 83, 87, 89, 95, 96, 98, 111, 116, 122, 137, 139, 143, 157, 158, 166, 167, 179, 186, 188, 194, 207, 218, 219, 224, 226, 228, 231, 236, 237, 238, 245, 249, 250, 252, 266, 269, 272, 280, 295, 297, 308, 312, 317, 320, 322, 324, 333, 334, 339, 343, 347, 349, 351, 356, 361, 369, 376, 379, 384, 385, 398, 406, 413, 414, 423, 426, 428, 431, 433, 436, 437, 447, 451, 460, 466, 470, 475, 476, 484, 488, 500, 502, 504, 506, 511, 514, 524, 525, 531, 532, 534, 539, 541, 542, 550, 552, 564, 570, 574, 593, 594, 603, 610, 615, 630, 632, 638, 641, 642, 643, 644, 651, 652, 657, 660, 663, 665, 666, 671, 673, 679, 680, 683, 687, 692, 698, 715, 716, 727, 732, 733, 743, 750, 753, 755, 767, 768, 771, 776, 780, 781, 789, 791, 794, 800, 806, 811, 815, 829, 840, 842, 844, 852, 855, 868, 878, 885, 886, 888, 895, 906, 914, 915, 916, 917, 921, 924, 928, 937, 945, 957, 967, 970, 983, 985, 990, 991])","title":"Untitled"},{"location":"examples/Untitled1/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }());","title":"Untitled1"},{"location":"examples/Untitled2/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import numpy as np from sklearn.datasets import make_moons import matplotlib.pyplot as plt from probaforms.models import RealNVP # generate sample X with conditions C X , y = make_moons ( n_samples = 1000 , noise = 0.1 ) C = y . reshape ( - 1 , 1 ) # fit nomalizing flow model model = RealNVP ( lr = 0.01 , n_epochs = 100 ) model . fit ( X , C ) # sample new objects X_gen = model . sample ( C ) # display the results plt . scatter ( X_gen [ y == 0 , 0 ], X_gen [ y == 0 , 1 ]) plt . scatter ( X_gen [ y == 1 , 0 ], X_gen [ y == 1 , 1 ]) plt . show () # generate sample X with conditions C X , y = make_moons ( n_samples = 1000 , noise = 0.1 ) C = y . reshape ( - 1 , 1 ) # fit nomalizing flow model model = RealNVP ( lr = 0.01 , n_epochs = 1000 ) model . fit ( C , X ) # sample new objects y_gen = model . sample ( X ) --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) Cell In[35], line 7 5 # fit nomalizing flow model 6 model = RealNVP(lr = 0.01 , n_epochs = 1000 ) ----> 7 model . fit ( C , X ) 9 # sample new objects 10 y_gen = model . sample(X) File ~/Documents/Github/probaforms/probaforms/models/realnvp.py:244 , in RealNVP.fit (self, X, C) 242 self . opt . zero_grad() 243 loss . backward() --> 244 self . opt . step ( ) 246 # caiculate and store loss 247 self . loss_history . append(loss . detach() . cpu()) File /opt/anaconda3/envs/probaforms/lib/python3.9/site-packages/torch/optim/optimizer.py:140 , in Optimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper (*args, **kwargs) 138 profile_name = \" Optimizer.step# {} .step \" . format(obj . __class__ . __name__ ) 139 with torch . autograd . profiler . record_function(profile_name): --> 140 out = func ( * args , * * kwargs ) 141 obj . _optimizer_step_code() 142 return out File /opt/anaconda3/envs/probaforms/lib/python3.9/site-packages/torch/optim/optimizer.py:23 , in _use_grad_for_differentiable.<locals>._use_grad (self, *args, **kwargs) 21 try : 22 torch . set_grad_enabled( self . defaults[ ' differentiable ' ]) ---> 23 ret = func ( self , * args , * * kwargs ) 24 finally : 25 torch . set_grad_enabled(prev_grad) File /opt/anaconda3/envs/probaforms/lib/python3.9/site-packages/torch/optim/adam.py:234 , in Adam.step (self, closure, grad_scaler) 231 raise RuntimeError ( ' `requires_grad` is not supported for `step` in differentiable mode ' ) 232 state_steps . append(state[ ' step ' ]) --> 234 adam ( params_with_grad , 235 grads , 236 exp_avgs , 237 exp_avg_sqs , 238 max_exp_avg_sqs , 239 state_steps , 240 amsgrad = group [ ' amsgrad ' ] , 241 beta1 = beta1 , 242 beta2 = beta2 , 243 lr = group [ ' lr ' ] , 244 weight_decay = group [ ' weight_decay ' ] , 245 eps = group [ ' eps ' ] , 246 maximize = group [ ' maximize ' ] , 247 foreach = group [ ' foreach ' ] , 248 capturable = group [ ' capturable ' ] , 249 differentiable = group [ ' differentiable ' ] , 250 fused = group [ ' fused ' ] , 251 grad_scale = grad_scale , 252 found_inf = found_inf ) 254 return loss File /opt/anaconda3/envs/probaforms/lib/python3.9/site-packages/torch/optim/adam.py:300 , in adam (params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize) 297 else : 298 func = _single_tensor_adam --> 300 func ( params , 301 grads , 302 exp_avgs , 303 exp_avg_sqs , 304 max_exp_avg_sqs , 305 state_steps , 306 amsgrad = amsgrad , 307 beta1 = beta1 , 308 beta2 = beta2 , 309 lr = lr , 310 weight_decay = weight_decay , 311 eps = eps , 312 maximize = maximize , 313 capturable = capturable , 314 differentiable = differentiable , 315 grad_scale = grad_scale , 316 found_inf = found_inf ) File /opt/anaconda3/envs/probaforms/lib/python3.9/site-packages/torch/optim/adam.py:363 , in _single_tensor_adam (params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable) 360 param = torch . view_as_real(param) 362 # Decay the first and second moment running average coefficient --> 363 exp_avg . mul_ ( beta1 ) . add_(grad, alpha = 1 - beta1) 364 exp_avg_sq . mul_(beta2) . addcmul_(grad, grad . conj(), value = 1 - beta2) 366 if capturable or differentiable: KeyboardInterrupt : bins = np . linspace ( - 1 , 2 , 101 ) plt . hist ( y_gen [ y == 0 , 0 ], alpha = 0.5 , bins = bins ) plt . hist ( y_gen [ y == 1 , 0 ], alpha = 0.5 , bins = bins ) plt . ylim ( 0 , 50 ) plt . show () import matplotlib.pyplot as plt import numpy as np def gen_two_samples ( dist , N ): sigma = np . array ([[ 1 , 0.7 ], [ 0.7 , 1 ]]) mu_x = np . array ([ 0 , 0 ]) mu_y = np . array ([ 0 , 0.5 ]) X = np . random . multivariate_normal ( mu_x , sigma , N ) Y = np . random . multivariate_normal ( mu_y , sigma , N ) return X , Y dist = 2 X , Y = gen_two_samples ( dist , N = 100 ) def plot_samples ( X , Y , dist = 0 ): plt . figure ( figsize = ( 6 , 4 )) plt . scatter ( X [:, 0 ], X [:, 1 ], label = 'X' , alpha = 0.5 , color = 'C0' ) plt . scatter ( Y [:, 0 ], Y [:, 1 ], label = 'Y' , alpha = 0.5 , color = 'C1' ) plt . title ( \"Distance = %.f\" % ( dist )) plt . legend () plt . tight_layout () plt . show () plot_samples ( X , Y , dist ) plt . hist ( X [:, 1 ], alpha = 0.5 , bins = 50 ) plt . hist ( Y [:, 1 ], alpha = 0.5 , bins = 50 ) plt . show () from probaforms import metrics metrics . maximum_mean_discrepancy ( X , Y , n_iters = 10 ) (0.059278931636857865, 0.01657768119865014) metrics . maximum_mean_discrepancy ( X [:, [ 0 ]], Y [:, [ 0 ]], n_iters = 10 ) (0.01906029106837901, 0.012591082301367205) metrics . maximum_mean_discrepancy ( X [:, [ 1 ]], Y [:, [ 1 ]], n_iters = 10 ) (0.0903250182228659, 0.05787955412327787)","title":"Untitled2"},{"location":"examples/Untitled3/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); # import of basic libraries import matplotlib.pyplot as plt from matplotlib import cm import pandas as pd import numpy as np A = np . random . normal ( 0 , 1 , ( 20 , 1 )) B = np . random . normal ( 0 , 5 , ( 20 , 1 )) AB = np . concatenate (( A , B ), axis = 0 ) Y = np . array ([[ 0 ]] * len ( A ) + [[ 1 ]] * len ( B ) ) from probaforms.metrics import frechet_distance frechet_distance ( AB [ Y [:, 0 ] == 0 ], AB [ Y [:, 0 ] == 1 ]) (24.32640088413341, 8.813339647396786) from probaforms.models import RealNVP nf = RealNVP ( lr = 0.01 , n_epochs = 100 ) nf . fit ( AB , Y ) plt . plot ( nf . loss_history ) [<matplotlib.lines.Line2D at 0x7fb836b72d30>] AB_hat = nf . sample ( Y ) frechet_distance ( AB_hat [ Y [:, 0 ] == 0 ], AB_hat [ Y [:, 0 ] == 1 ]) (33.567800736259116, 9.329396096961407) x = [] y = [] for i in range ( 100 ): ab = nf . sample ( Y ) x . append ( ab ) y . append ( Y ) AB_hat = np . concatenate ( tuple ( x ), axis = 0 ) Y_hat = np . concatenate ( tuple ( y ), axis = 0 ) AB_hat . shape (4000, 1) frechet_distance ( AB_hat [ Y_hat [:, 0 ] == 0 ], AB_hat [ Y_hat [:, 0 ] == 1 ]) (26.955687591325674, 1.0641249673411826) plt . hist ( AB_hat [ Y_hat == 0 ], alpha = 0.5 , density = True ) plt . hist ( AB_hat [ Y_hat == 1 ], alpha = 0.5 , density = True ) plt . hist ( AB [ Y == 0 ], histtype = 'step' , density = True ) plt . hist ( AB [ Y == 1 ], histtype = 'step' , density = True ) plt . show ()","title":"Untitled3"},{"location":"examples/forecast/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Probabilistic Time Series Forecast In this example, we will consider how to use conditional generative models for probabilistic time series forecast. The generative models help to estimate confidence intervals of the predictions. import matplotlib.pyplot as plt import pandas as pd import numpy as np Read data set We will predict the number of international airline passengers in units of 1,000. The data ranges from January 1949 to December 1960, or 12 years, with 144 observations. # data download ! wget https : // raw . githubusercontent . com / jbrownlee / Datasets / master / airline - passengers . csv # read the data data = pd . read_csv ( \"airline-passengers.csv\" ) data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Month Passengers 0 1949-01 112 1 1949-02 118 2 1949-03 132 3 1949-04 129 4 1949-05 121 Data visualization # get a time series Y = data [[ 'Passengers' ]] . values plt . figure ( figsize = ( 12 , 4 )) plt . plot ( Y , label = 'True' ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . tight_layout () plt . show () Preprocessing # scaling from sklearn.preprocessing import StandardScaler Y = StandardScaler () . fit_transform ( Y ) plt . figure ( figsize = ( 12 , 4 )) plt . plot ( Y , label = 'True' ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . tight_layout () plt . show () Autoregression model for time series forecast Consider y_1, y_2, ..., y_i, ..., y_N are observations of a time series. Autoegression model (AR) for the time series forecast assumes the following: \\hat{y}_{i+m} = f( y_{i}, y_{i-1}, ... y_{i-k+1} ) where \\hat{y}_{i+m} is a predicted value. In matrix form we will define this model as: \\hat{Y} = f(X) where X = \\left( \\begin{array}{cccc} y_{k} & y_{k-1} & \\ldots & y_{1}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ y_{k+j} & y_{k+j-1} & \\ldots & y_{j+1}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\end{array} \\right) Y = \\left( \\begin{array}{c} y_{k+m} \\\\ \\vdots\\\\ y_{k+j+m}\\\\ \\vdots \\end{array} \\right) K = 10 M = 10 def AR_matrices ( Y , K , M ): X_AR = [] Y_AR = [] for i in range ( len ( Y )): if i < K - 1 : continue if i + M >= len ( Y ): break ax_ar = Y [ i + 1 - K : i + 1 ] . reshape ( - 1 , ) X_AR . append ( ax_ar ) ay_ar = Y [ i + M ] #[0] # predict only y_{i+M} # ay_ar = Y[i+1:i+M+1].reshape(-1, ) # predict y_{i+M}, ..., y_{i+2}, y_{i+1} Y_AR . append ( ay_ar ) return np . array ( X_AR ), np . array ( Y_AR ) # prepare X and Y matrices X_AR , Y_AR = AR_matrices ( Y , K , M ) X_AR [: 2 ] array([[-1.40777884, -1.35759023, -1.24048348, -1.26557778, -1.33249593, -1.21538918, -1.10664719, -1.10664719, -1.20702441, -1.34922546], [-1.35759023, -1.24048348, -1.26557778, -1.33249593, -1.21538918, -1.10664719, -1.10664719, -1.20702441, -1.34922546, -1.47469699]]) Y_AR [: 2 ] array([[-0.9226223 ], [-1.02299951]]) X_AR . shape , Y_AR . shape ((125, 10), (125, 1)) Train and test split N = 90 X_AR_train , X_AR_test = X_AR [: N ], X_AR [ N :] Y_AR_train , Y_AR_test = Y_AR [: N ], Y_AR [ N :] Fit probabilistic model from probaforms.models import RealNVP # fit nomalizing flow model model = RealNVP ( lr = 0.01 , n_epochs = 100 , weight_decay = 0.2 ) model . fit ( Y_AR_train , X_AR_train ) plt . figure ( figsize = ( 12 , 4 )) plt . plot ( model . loss_history ) plt . xlabel ( 'Number of iterations' ) plt . ylabel ( 'Loss' ) plt . tight_layout () plt . show () Single prediction # sample new objects Y_pred_test = model . sample ( X_AR_test ) plt . figure ( figsize = ( 12 , 4 )) plt . plot ( Y_AR_test [:, - 1 ], label = 'True' , alpha = 1. ) plt . plot ( Y_pred_test [:, - 1 ], label = 'Prediction' , alpha = 1. ) plt . legend ( loc = 'best' ) plt . tight_layout () plt . show () Multiple predictions predictions_test = [] for i in range ( 1000 ): Y_pred_test = model . sample ( X_AR_test ) # store predictions predictions_test . append ( Y_pred_test [:, - 1 ]) predictions_test = np . array ( predictions_test ) plt . figure ( figsize = ( 12 , 4 )) # mean prediction plt . plot ( predictions_test . mean ( axis = 0 ), label = 'Prediction (mean)' , alpha = 1. , color = '0' ) # 90% CI xx = np . arange ( len ( Y_pred_test )) y_top = np . quantile ( predictions_test , q = 0.95 , axis = 0 ) y_bot = np . quantile ( predictions_test , q = 0.05 , axis = 0 ) plt . fill_between ( xx , y_bot , y_top , label = \"90% CI\" , alpha = 0.5 , color = 'C0' ) # true plt . plot ( Y_AR_test [:, - 1 ], label = 'True' , alpha = 1. , color = 'C1' ) plt . legend ( loc = 'best' ) plt . tight_layout () plt . show ()","title":"Time Series Forecast"},{"location":"examples/forecast/#probabilistic-time-series-forecast","text":"In this example, we will consider how to use conditional generative models for probabilistic time series forecast. The generative models help to estimate confidence intervals of the predictions. import matplotlib.pyplot as plt import pandas as pd import numpy as np","title":"Probabilistic Time Series Forecast"},{"location":"examples/forecast/#read-data-set","text":"We will predict the number of international airline passengers in units of 1,000. The data ranges from January 1949 to December 1960, or 12 years, with 144 observations. # data download ! wget https : // raw . githubusercontent . com / jbrownlee / Datasets / master / airline - passengers . csv # read the data data = pd . read_csv ( \"airline-passengers.csv\" ) data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Month Passengers 0 1949-01 112 1 1949-02 118 2 1949-03 132 3 1949-04 129 4 1949-05 121","title":"Read data set"},{"location":"examples/forecast/#data-visualization","text":"# get a time series Y = data [[ 'Passengers' ]] . values plt . figure ( figsize = ( 12 , 4 )) plt . plot ( Y , label = 'True' ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . tight_layout () plt . show ()","title":"Data visualization"},{"location":"examples/forecast/#preprocessing","text":"# scaling from sklearn.preprocessing import StandardScaler Y = StandardScaler () . fit_transform ( Y ) plt . figure ( figsize = ( 12 , 4 )) plt . plot ( Y , label = 'True' ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . tight_layout () plt . show ()","title":"Preprocessing"},{"location":"examples/forecast/#autoregression-model-for-time-series-forecast","text":"Consider y_1, y_2, ..., y_i, ..., y_N are observations of a time series. Autoegression model (AR) for the time series forecast assumes the following: \\hat{y}_{i+m} = f( y_{i}, y_{i-1}, ... y_{i-k+1} ) where \\hat{y}_{i+m} is a predicted value. In matrix form we will define this model as: \\hat{Y} = f(X) where X = \\left( \\begin{array}{cccc} y_{k} & y_{k-1} & \\ldots & y_{1}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ y_{k+j} & y_{k+j-1} & \\ldots & y_{j+1}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\end{array} \\right) Y = \\left( \\begin{array}{c} y_{k+m} \\\\ \\vdots\\\\ y_{k+j+m}\\\\ \\vdots \\end{array} \\right) K = 10 M = 10 def AR_matrices ( Y , K , M ): X_AR = [] Y_AR = [] for i in range ( len ( Y )): if i < K - 1 : continue if i + M >= len ( Y ): break ax_ar = Y [ i + 1 - K : i + 1 ] . reshape ( - 1 , ) X_AR . append ( ax_ar ) ay_ar = Y [ i + M ] #[0] # predict only y_{i+M} # ay_ar = Y[i+1:i+M+1].reshape(-1, ) # predict y_{i+M}, ..., y_{i+2}, y_{i+1} Y_AR . append ( ay_ar ) return np . array ( X_AR ), np . array ( Y_AR ) # prepare X and Y matrices X_AR , Y_AR = AR_matrices ( Y , K , M ) X_AR [: 2 ] array([[-1.40777884, -1.35759023, -1.24048348, -1.26557778, -1.33249593, -1.21538918, -1.10664719, -1.10664719, -1.20702441, -1.34922546], [-1.35759023, -1.24048348, -1.26557778, -1.33249593, -1.21538918, -1.10664719, -1.10664719, -1.20702441, -1.34922546, -1.47469699]]) Y_AR [: 2 ] array([[-0.9226223 ], [-1.02299951]]) X_AR . shape , Y_AR . shape ((125, 10), (125, 1))","title":"Autoregression model for time series forecast"},{"location":"examples/forecast/#train-and-test-split","text":"N = 90 X_AR_train , X_AR_test = X_AR [: N ], X_AR [ N :] Y_AR_train , Y_AR_test = Y_AR [: N ], Y_AR [ N :]","title":"Train and test split"},{"location":"examples/forecast/#fit-probabilistic-model","text":"from probaforms.models import RealNVP # fit nomalizing flow model model = RealNVP ( lr = 0.01 , n_epochs = 100 , weight_decay = 0.2 ) model . fit ( Y_AR_train , X_AR_train ) plt . figure ( figsize = ( 12 , 4 )) plt . plot ( model . loss_history ) plt . xlabel ( 'Number of iterations' ) plt . ylabel ( 'Loss' ) plt . tight_layout () plt . show ()","title":"Fit probabilistic model"},{"location":"examples/forecast/#single-prediction","text":"# sample new objects Y_pred_test = model . sample ( X_AR_test ) plt . figure ( figsize = ( 12 , 4 )) plt . plot ( Y_AR_test [:, - 1 ], label = 'True' , alpha = 1. ) plt . plot ( Y_pred_test [:, - 1 ], label = 'Prediction' , alpha = 1. ) plt . legend ( loc = 'best' ) plt . tight_layout () plt . show ()","title":"Single prediction"},{"location":"examples/forecast/#multiple-predictions","text":"predictions_test = [] for i in range ( 1000 ): Y_pred_test = model . sample ( X_AR_test ) # store predictions predictions_test . append ( Y_pred_test [:, - 1 ]) predictions_test = np . array ( predictions_test ) plt . figure ( figsize = ( 12 , 4 )) # mean prediction plt . plot ( predictions_test . mean ( axis = 0 ), label = 'Prediction (mean)' , alpha = 1. , color = '0' ) # 90% CI xx = np . arange ( len ( Y_pred_test )) y_top = np . quantile ( predictions_test , q = 0.95 , axis = 0 ) y_bot = np . quantile ( predictions_test , q = 0.05 , axis = 0 ) plt . fill_between ( xx , y_bot , y_top , label = \"90% CI\" , alpha = 0.5 , color = 'C0' ) # true plt . plot ( Y_AR_test [:, - 1 ], label = 'True' , alpha = 1. , color = 'C1' ) plt . legend ( loc = 'best' ) plt . tight_layout () plt . show ()","title":"Multiple predictions"},{"location":"examples/metrics/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Metrics usage This example shows how to use metrics to measure the quality of the generated samples. The metrics take two samples (real and generated) and estimate discrepancies between their distributions. The smaller metrics values the better. Generate two samples Let's generate two samples from multivariate normal distributions with the same covariance matrices, but with some distance between their means. import matplotlib.pyplot as plt import numpy as np from probaforms import metrics def gen_two_samples ( dist , N ): sigma = np . array ([[ 1 , 0.7 ], [ 0.7 , 1 ]]) mu_x = np . array ([ 0 , 0 ]) mu_y = mu_x + dist / np . sqrt ( 2 ) X = np . random . multivariate_normal ( mu_x , sigma , N ) Y = np . random . multivariate_normal ( mu_y , sigma , N ) return X , Y # generate two samples with a size of 1000 dist = 2 X , Y = gen_two_samples ( dist , N = 1000 ) def plot_samples ( X , Y , dist = 0 ): plt . figure ( figsize = ( 6 , 4 )) plt . scatter ( X [:, 0 ], X [:, 1 ], label = 'X' , alpha = 0.5 , color = 'C0' ) plt . scatter ( Y [:, 0 ], Y [:, 1 ], label = 'Y' , alpha = 0.5 , color = 'C1' ) plt . title ( \"Distance = %.f\" % ( dist )) plt . legend () plt . tight_layout () plt . show () plot_samples ( X , Y , dist ) Compute discrepancies between the samples def get_metrics ( X , Y ): mu , sigma = metrics . frechet_distance ( X , Y ) print ( r \"Frechet Distance = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . kolmogorov_smirnov_1d ( X , Y ) print ( r \"Kolmogorov-Smirnov = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . cramer_von_mises_1d ( X , Y ) print ( r \"Cramer-von Mises = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . anderson_darling_1d ( X , Y ) print ( r \"Anderson-Darling = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . roc_auc_score_1d ( X , Y ) print ( r \"ROC AUC = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . kullback_leibler_1d_kde ( X , Y ) print ( r \"Kullback-Leibler KDE = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . jensen_shannon_1d_kde ( X , Y ) print ( r \"Jensen-Shannon KDE = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . maximum_mean_discrepancy ( X , Y ) print ( r \"Maximum Mean Discrepancy = %.6f +- %.6f \" % ( mu , sigma )) get_metrics ( X , Y ) Frechet Distance = 3.711472 +- 0.240218 Kolmogorov-Smirnov = 0.509375 +- 0.015197 Cramer-von Mises = 64.544793 +- 3.777022 Anderson-Darling = 423.161173 +- 20.731482 ROC AUC = 0.824993 +- 0.007886 Kullback-Leibler KDE = 0.852085 +- 0.063555 Jensen-Shannon KDE = 0.173695 +- 0.009267 Maximum Mean Discrepancy = 0.305732 +- 0.019054 Additional experiments with other distances dist = 10. X , Y = gen_two_samples ( dist , N = 1000 ) plot_samples ( X , Y , dist ) get_metrics ( X , Y ) Frechet Distance = 100.640957 +- 1.078155 Kolmogorov-Smirnov = 1.000000 +- 0.000000 Cramer-von Mises = 166.667082 +- 0.000017 Anderson-Darling = 1015.172596 +- 0.022680 ROC AUC = 1.000000 +- 0.000000 Kullback-Leibler KDE = 12.633292 +- 0.031014 Jensen-Shannon KDE = 0.692143 +- 0.000470 Maximum Mean Discrepancy = 1.502160 +- 0.007987 dist = 0. X , Y = gen_two_samples ( dist , N = 1000 ) plot_samples ( X , Y , dist ) get_metrics ( X , Y ) Frechet Distance = 0.007647 +- 0.005009 Kolmogorov-Smirnov = 0.053815 +- 0.010366 Cramer-von Mises = 0.325126 +- 0.186177 Anderson-Darling = 0.955208 +- 0.973257 ROC AUC = 0.513250 +- 0.005923 Kullback-Leibler KDE = 0.011113 +- 0.003471 Jensen-Shannon KDE = 0.002718 +- 0.000755 Maximum Mean Discrepancy = 0.001662 +- 0.001076","title":"Metrics usage"},{"location":"examples/metrics/#metrics-usage","text":"This example shows how to use metrics to measure the quality of the generated samples. The metrics take two samples (real and generated) and estimate discrepancies between their distributions. The smaller metrics values the better.","title":"Metrics usage"},{"location":"examples/metrics/#generate-two-samples","text":"Let's generate two samples from multivariate normal distributions with the same covariance matrices, but with some distance between their means. import matplotlib.pyplot as plt import numpy as np from probaforms import metrics def gen_two_samples ( dist , N ): sigma = np . array ([[ 1 , 0.7 ], [ 0.7 , 1 ]]) mu_x = np . array ([ 0 , 0 ]) mu_y = mu_x + dist / np . sqrt ( 2 ) X = np . random . multivariate_normal ( mu_x , sigma , N ) Y = np . random . multivariate_normal ( mu_y , sigma , N ) return X , Y # generate two samples with a size of 1000 dist = 2 X , Y = gen_two_samples ( dist , N = 1000 ) def plot_samples ( X , Y , dist = 0 ): plt . figure ( figsize = ( 6 , 4 )) plt . scatter ( X [:, 0 ], X [:, 1 ], label = 'X' , alpha = 0.5 , color = 'C0' ) plt . scatter ( Y [:, 0 ], Y [:, 1 ], label = 'Y' , alpha = 0.5 , color = 'C1' ) plt . title ( \"Distance = %.f\" % ( dist )) plt . legend () plt . tight_layout () plt . show () plot_samples ( X , Y , dist )","title":"Generate two samples"},{"location":"examples/metrics/#compute-discrepancies-between-the-samples","text":"def get_metrics ( X , Y ): mu , sigma = metrics . frechet_distance ( X , Y ) print ( r \"Frechet Distance = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . kolmogorov_smirnov_1d ( X , Y ) print ( r \"Kolmogorov-Smirnov = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . cramer_von_mises_1d ( X , Y ) print ( r \"Cramer-von Mises = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . anderson_darling_1d ( X , Y ) print ( r \"Anderson-Darling = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . roc_auc_score_1d ( X , Y ) print ( r \"ROC AUC = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . kullback_leibler_1d_kde ( X , Y ) print ( r \"Kullback-Leibler KDE = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . jensen_shannon_1d_kde ( X , Y ) print ( r \"Jensen-Shannon KDE = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . maximum_mean_discrepancy ( X , Y ) print ( r \"Maximum Mean Discrepancy = %.6f +- %.6f \" % ( mu , sigma )) get_metrics ( X , Y ) Frechet Distance = 3.711472 +- 0.240218 Kolmogorov-Smirnov = 0.509375 +- 0.015197 Cramer-von Mises = 64.544793 +- 3.777022 Anderson-Darling = 423.161173 +- 20.731482 ROC AUC = 0.824993 +- 0.007886 Kullback-Leibler KDE = 0.852085 +- 0.063555 Jensen-Shannon KDE = 0.173695 +- 0.009267 Maximum Mean Discrepancy = 0.305732 +- 0.019054","title":"Compute discrepancies between the samples"},{"location":"examples/metrics/#additional-experiments-with-other-distances","text":"dist = 10. X , Y = gen_two_samples ( dist , N = 1000 ) plot_samples ( X , Y , dist ) get_metrics ( X , Y ) Frechet Distance = 100.640957 +- 1.078155 Kolmogorov-Smirnov = 1.000000 +- 0.000000 Cramer-von Mises = 166.667082 +- 0.000017 Anderson-Darling = 1015.172596 +- 0.022680 ROC AUC = 1.000000 +- 0.000000 Kullback-Leibler KDE = 12.633292 +- 0.031014 Jensen-Shannon KDE = 0.692143 +- 0.000470 Maximum Mean Discrepancy = 1.502160 +- 0.007987 dist = 0. X , Y = gen_two_samples ( dist , N = 1000 ) plot_samples ( X , Y , dist ) get_metrics ( X , Y ) Frechet Distance = 0.007647 +- 0.005009 Kolmogorov-Smirnov = 0.053815 +- 0.010366 Cramer-von Mises = 0.325126 +- 0.186177 Anderson-Darling = 0.955208 +- 0.973257 ROC AUC = 0.513250 +- 0.005923 Kullback-Leibler KDE = 0.011113 +- 0.003471 Jensen-Shannon KDE = 0.002718 +- 0.000755 Maximum Mean Discrepancy = 0.001662 +- 0.001076","title":"Additional experiments with other distances"},{"location":"examples/regression-Copy1/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Regression Generate data set In this example, we will solve a regression task with uncertainty estimation using Conditional Normalizing Flows. In addition, we will compare this approach with the Gaussian Process. Let\u2019s generate a data set with observations sampled from a normal distribution, where mean and variance depend on input variable X. # import of basic libraries import matplotlib.pyplot as plt from matplotlib import cm import pandas as pd import numpy as np # function we would like to predict def func ( X ): return np . exp ( - X ) # input variable X = np . linspace ( 0 , 5 , 500 ) . reshape ( - 1 , 1 ) # mean values of targets y mu = func ( X ) # normal noise eps = np . random . normal ( 0 , 1 , X . shape ) sigma = 0.05 * ( X + 0.5 ) # target variable we need to predict y = mu + eps * sigma plt . figure ( figsize = ( 12 , 6 )) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show () Normalizing flows Now, we use Conditional Real NVP to learn conditional distribution p(y|X) of the observations. Here, X is condition, y is our target to learn its distribution. from probaforms.models import RealNVP , CVAE , ConditionalWGAN # fit nomalizing flow model model = ConditionalWGAN () model . fit ( y , X ) # (target, condition) # sample new observations y_gen = model . sample ( X ) plt . plot ( model . disc_loss_history ) plt . show () plt . plot ( model . gen_loss_history ) plt . show () The figure below shows that Normalizing Flow (NF) successfully learnt the distribution p(y|X) . We can use NF to sample new objects from the distribution that are similar to the real observations. plt . figure ( figsize = ( 12 , 6 )) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y_gen , marker = '+' , label = 'Generated with NF' , color = 'C1' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , linewidth = 2 ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show () plt . hist ( y_gen ) plt . hist ( y ) (array([ 9., 26., 48., 97., 127., 84., 40., 31., 19., 19.]), array([-0.53246506, -0.3784361 , -0.22440714, -0.07037817, 0.08365079, 0.23767975, 0.39170871, 0.54573768, 0.69976664, 0.8537956 , 1.00782457]), <BarContainer object of 10 artists>) Now, we will repeat the sampling procedure several times and empirically estimate the mean value of the function and its standard deviation. The standard deviation we consider as the prediction uncertainty. The figure below demonstrates that NF successfully estimates the mean and standard deviation of the sample. y_preds = [] # repeat sampling several times for i in range ( 1000 ): # sample with NF y_gen = model . sample ( X ) y_preds . append ( y_gen ) y_preds = np . array ( y_preds ) # estimate the mean of the predictions mu_pred = y_preds . mean ( axis = 0 ) . reshape ( - 1 ,) # estimate the standard deviation of the predictions sigma_pred = y_preds . std ( axis = 0 ) . reshape ( - 1 ,) plt . figure ( figsize = ( 12 , 6 )) plt . fill_between ( X [:, 0 ], y1 = mu_pred + sigma_pred , y2 = mu_pred - sigma_pred , color = 'C1' , alpha = 0.5 , label = r '$\\mu \\pm \\sigma$ by NF' ) plt . plot ( X , mu_pred , label = 'Mean by NF' , color = 'C1' , linewidth = 4 ) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , color = 'C0' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show () Gaussian Process Finally, let\u2019s solve the same task with the Gaussian Process (GP) to compare the results. The plot below shows that GP predicts the mean with good precision. However, it is not able to learn the dependency of the standard deviation from the input variable X. from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import WhiteKernel , RBF , ConstantKernel as C # define kernel kernel = C () * RBF () + WhiteKernel () # fit GP gpr = GaussianProcessRegressor ( kernel = kernel ) gpr . fit ( X , y ) # make predictions of the means and standard deviations mu_pred_gp , sigma_pred_gp = gpr . predict ( X , return_std = True ) plt . figure ( figsize = ( 12 , 6 )) plt . fill_between ( X [:, 0 ], y1 = mu_pred_gp + sigma_pred_gp , y2 = mu_pred_gp - sigma_pred_gp , color = 'C1' , alpha = 0.5 , label = r '$\\mu \\pm \\sigma$ by GP' ) plt . plot ( X , mu_pred_gp , label = 'Mean by GP' , color = 'C1' , linewidth = 4 ) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , color = 'C0' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show ()","title":"regression Copy1"},{"location":"examples/regression-Copy1/#regression","text":"","title":"Regression"},{"location":"examples/regression-Copy1/#generate-data-set","text":"In this example, we will solve a regression task with uncertainty estimation using Conditional Normalizing Flows. In addition, we will compare this approach with the Gaussian Process. Let\u2019s generate a data set with observations sampled from a normal distribution, where mean and variance depend on input variable X. # import of basic libraries import matplotlib.pyplot as plt from matplotlib import cm import pandas as pd import numpy as np # function we would like to predict def func ( X ): return np . exp ( - X ) # input variable X = np . linspace ( 0 , 5 , 500 ) . reshape ( - 1 , 1 ) # mean values of targets y mu = func ( X ) # normal noise eps = np . random . normal ( 0 , 1 , X . shape ) sigma = 0.05 * ( X + 0.5 ) # target variable we need to predict y = mu + eps * sigma plt . figure ( figsize = ( 12 , 6 )) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show ()","title":"Generate data set"},{"location":"examples/regression-Copy1/#normalizing-flows","text":"Now, we use Conditional Real NVP to learn conditional distribution p(y|X) of the observations. Here, X is condition, y is our target to learn its distribution. from probaforms.models import RealNVP , CVAE , ConditionalWGAN # fit nomalizing flow model model = ConditionalWGAN () model . fit ( y , X ) # (target, condition) # sample new observations y_gen = model . sample ( X ) plt . plot ( model . disc_loss_history ) plt . show () plt . plot ( model . gen_loss_history ) plt . show () The figure below shows that Normalizing Flow (NF) successfully learnt the distribution p(y|X) . We can use NF to sample new objects from the distribution that are similar to the real observations. plt . figure ( figsize = ( 12 , 6 )) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y_gen , marker = '+' , label = 'Generated with NF' , color = 'C1' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , linewidth = 2 ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show () plt . hist ( y_gen ) plt . hist ( y ) (array([ 9., 26., 48., 97., 127., 84., 40., 31., 19., 19.]), array([-0.53246506, -0.3784361 , -0.22440714, -0.07037817, 0.08365079, 0.23767975, 0.39170871, 0.54573768, 0.69976664, 0.8537956 , 1.00782457]), <BarContainer object of 10 artists>) Now, we will repeat the sampling procedure several times and empirically estimate the mean value of the function and its standard deviation. The standard deviation we consider as the prediction uncertainty. The figure below demonstrates that NF successfully estimates the mean and standard deviation of the sample. y_preds = [] # repeat sampling several times for i in range ( 1000 ): # sample with NF y_gen = model . sample ( X ) y_preds . append ( y_gen ) y_preds = np . array ( y_preds ) # estimate the mean of the predictions mu_pred = y_preds . mean ( axis = 0 ) . reshape ( - 1 ,) # estimate the standard deviation of the predictions sigma_pred = y_preds . std ( axis = 0 ) . reshape ( - 1 ,) plt . figure ( figsize = ( 12 , 6 )) plt . fill_between ( X [:, 0 ], y1 = mu_pred + sigma_pred , y2 = mu_pred - sigma_pred , color = 'C1' , alpha = 0.5 , label = r '$\\mu \\pm \\sigma$ by NF' ) plt . plot ( X , mu_pred , label = 'Mean by NF' , color = 'C1' , linewidth = 4 ) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , color = 'C0' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show ()","title":"Normalizing flows"},{"location":"examples/regression-Copy1/#gaussian-process","text":"Finally, let\u2019s solve the same task with the Gaussian Process (GP) to compare the results. The plot below shows that GP predicts the mean with good precision. However, it is not able to learn the dependency of the standard deviation from the input variable X. from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import WhiteKernel , RBF , ConstantKernel as C # define kernel kernel = C () * RBF () + WhiteKernel () # fit GP gpr = GaussianProcessRegressor ( kernel = kernel ) gpr . fit ( X , y ) # make predictions of the means and standard deviations mu_pred_gp , sigma_pred_gp = gpr . predict ( X , return_std = True ) plt . figure ( figsize = ( 12 , 6 )) plt . fill_between ( X [:, 0 ], y1 = mu_pred_gp + sigma_pred_gp , y2 = mu_pred_gp - sigma_pred_gp , color = 'C1' , alpha = 0.5 , label = r '$\\mu \\pm \\sigma$ by GP' ) plt . plot ( X , mu_pred_gp , label = 'Mean by GP' , color = 'C1' , linewidth = 4 ) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , color = 'C0' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show ()","title":"Gaussian Process"},{"location":"examples/regression-Copy2/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Regression Generate data set In this example, we will solve a regression task with uncertainty estimation using Conditional Normalizing Flows. In addition, we will compare this approach with the Gaussian Process. Let\u2019s generate a data set with observations sampled from a normal distribution, where mean and variance depend on input variable X. # import of basic libraries import matplotlib.pyplot as plt from matplotlib import cm import pandas as pd import numpy as np # function we would like to predict def func ( X ): return np . exp ( - X ) # input variable X = np . linspace ( 0 , 5 , 500 ) . reshape ( - 1 , 1 ) # mean values of targets y mu = func ( X ) # normal noise eps = np . random . normal ( 0 , 1 , X . shape ) sigma = 0.05 * ( X + 0.5 ) # target variable we need to predict y = mu + eps * sigma plt . figure ( figsize = ( 12 , 6 )) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show () Normalizing flows Now, we use Conditional Real NVP to learn conditional distribution p(y|X) of the observations. Here, X is condition, y is our target to learn its distribution. from probaforms.models import RealNVP # fit nomalizing flow model model = RealNVP ( lr = 0.01 , n_epochs = 100 ) model . fit ( y , X ) # (target, condition) # sample new observations y_gen = model . sample ( X ) The figure below shows that Normalizing Flow (NF) successfully learnt the distribution p(y|X) . We can use NF to sample new objects from the distribution that are similar to the real observations. plt . figure ( figsize = ( 12 , 6 )) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y_gen , marker = '+' , label = 'Generated with NF' , color = 'C1' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , linewidth = 2 ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show () Now, we will repeat the sampling procedure several times and empirically estimate the mean value of the function and its standard deviation. The standard deviation we consider as the prediction uncertainty. The figure below demonstrates that NF successfully estimates the mean and standard deviation of the sample. y_preds = [] # repeat sampling several times for i in range ( 1000 ): # sample with NF y_gen = model . sample ( X ) y_preds . append ( y_gen ) y_preds = np . array ( y_preds ) # estimate the mean of the predictions mu_pred = y_preds . mean ( axis = 0 ) . reshape ( - 1 ,) # estimate the standard deviation of the predictions sigma_pred = y_preds . std ( axis = 0 ) . reshape ( - 1 ,) plt . figure ( figsize = ( 12 , 6 )) plt . fill_between ( X [:, 0 ], y1 = mu_pred + sigma_pred , y2 = mu_pred - sigma_pred , color = 'C1' , alpha = 0.5 , label = r '$\\mu \\pm \\sigma$ by NF' ) plt . plot ( X , mu_pred , label = 'Mean by NF' , color = 'C1' , linewidth = 4 ) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , color = 'C0' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show () Gaussian Process Finally, let\u2019s solve the same task with the Gaussian Process (GP) to compare the results. The plot below shows that GP predicts the mean with good precision. However, it is not able to learn the dependency of the standard deviation from the input variable X. from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import WhiteKernel , RBF , ConstantKernel as C # define kernel kernel = C () * RBF () + WhiteKernel () # fit GP gpr = GaussianProcessRegressor ( kernel = kernel ) gpr . fit ( X , y ) # make predictions of the means and standard deviations mu_pred_gp , sigma_pred_gp = gpr . predict ( X , return_std = True ) plt . figure ( figsize = ( 12 , 6 )) plt . fill_between ( X [:, 0 ], y1 = mu_pred_gp + sigma_pred_gp , y2 = mu_pred_gp - sigma_pred_gp , color = 'C1' , alpha = 0.5 , label = r '$\\mu \\pm \\sigma$ by GP' ) plt . plot ( X , mu_pred_gp , label = 'Mean by GP' , color = 'C1' , linewidth = 4 ) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , color = 'C0' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show ()","title":"regression Copy2"},{"location":"examples/regression-Copy2/#regression","text":"","title":"Regression"},{"location":"examples/regression-Copy2/#generate-data-set","text":"In this example, we will solve a regression task with uncertainty estimation using Conditional Normalizing Flows. In addition, we will compare this approach with the Gaussian Process. Let\u2019s generate a data set with observations sampled from a normal distribution, where mean and variance depend on input variable X. # import of basic libraries import matplotlib.pyplot as plt from matplotlib import cm import pandas as pd import numpy as np # function we would like to predict def func ( X ): return np . exp ( - X ) # input variable X = np . linspace ( 0 , 5 , 500 ) . reshape ( - 1 , 1 ) # mean values of targets y mu = func ( X ) # normal noise eps = np . random . normal ( 0 , 1 , X . shape ) sigma = 0.05 * ( X + 0.5 ) # target variable we need to predict y = mu + eps * sigma plt . figure ( figsize = ( 12 , 6 )) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show ()","title":"Generate data set"},{"location":"examples/regression-Copy2/#normalizing-flows","text":"Now, we use Conditional Real NVP to learn conditional distribution p(y|X) of the observations. Here, X is condition, y is our target to learn its distribution. from probaforms.models import RealNVP # fit nomalizing flow model model = RealNVP ( lr = 0.01 , n_epochs = 100 ) model . fit ( y , X ) # (target, condition) # sample new observations y_gen = model . sample ( X ) The figure below shows that Normalizing Flow (NF) successfully learnt the distribution p(y|X) . We can use NF to sample new objects from the distribution that are similar to the real observations. plt . figure ( figsize = ( 12 , 6 )) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y_gen , marker = '+' , label = 'Generated with NF' , color = 'C1' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , linewidth = 2 ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show () Now, we will repeat the sampling procedure several times and empirically estimate the mean value of the function and its standard deviation. The standard deviation we consider as the prediction uncertainty. The figure below demonstrates that NF successfully estimates the mean and standard deviation of the sample. y_preds = [] # repeat sampling several times for i in range ( 1000 ): # sample with NF y_gen = model . sample ( X ) y_preds . append ( y_gen ) y_preds = np . array ( y_preds ) # estimate the mean of the predictions mu_pred = y_preds . mean ( axis = 0 ) . reshape ( - 1 ,) # estimate the standard deviation of the predictions sigma_pred = y_preds . std ( axis = 0 ) . reshape ( - 1 ,) plt . figure ( figsize = ( 12 , 6 )) plt . fill_between ( X [:, 0 ], y1 = mu_pred + sigma_pred , y2 = mu_pred - sigma_pred , color = 'C1' , alpha = 0.5 , label = r '$\\mu \\pm \\sigma$ by NF' ) plt . plot ( X , mu_pred , label = 'Mean by NF' , color = 'C1' , linewidth = 4 ) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , color = 'C0' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show ()","title":"Normalizing flows"},{"location":"examples/regression-Copy2/#gaussian-process","text":"Finally, let\u2019s solve the same task with the Gaussian Process (GP) to compare the results. The plot below shows that GP predicts the mean with good precision. However, it is not able to learn the dependency of the standard deviation from the input variable X. from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import WhiteKernel , RBF , ConstantKernel as C # define kernel kernel = C () * RBF () + WhiteKernel () # fit GP gpr = GaussianProcessRegressor ( kernel = kernel ) gpr . fit ( X , y ) # make predictions of the means and standard deviations mu_pred_gp , sigma_pred_gp = gpr . predict ( X , return_std = True ) plt . figure ( figsize = ( 12 , 6 )) plt . fill_between ( X [:, 0 ], y1 = mu_pred_gp + sigma_pred_gp , y2 = mu_pred_gp - sigma_pred_gp , color = 'C1' , alpha = 0.5 , label = r '$\\mu \\pm \\sigma$ by GP' ) plt . plot ( X , mu_pred_gp , label = 'Mean by GP' , color = 'C1' , linewidth = 4 ) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , color = 'C0' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show ()","title":"Gaussian Process"},{"location":"examples/regression/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Regression Generate data set In this example, we will solve a regression task with uncertainty estimation using Conditional Normalizing Flows. In addition, we will compare this approach with the Gaussian Process. Let\u2019s generate a data set with observations sampled from a normal distribution, where mean and variance depend on input variable X. # import of basic libraries import matplotlib.pyplot as plt from matplotlib import cm import pandas as pd import numpy as np # function we would like to predict def func ( X ): return np . exp ( - X ) # input variable X = np . linspace ( 0 , 5 , 500 ) . reshape ( - 1 , 1 ) # mean values of targets y mu = func ( X ) # normal noise eps = np . random . normal ( 0 , 1 , X . shape ) sigma = 0.05 * ( X + 0.5 ) # target variable we need to predict y = mu + eps * sigma plt . figure ( figsize = ( 12 , 6 )) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show () Normalizing flows Now, we use Conditional Real NVP to learn conditional distribution p(y|X) of the observations. Here, X is condition, y is our target to learn its distribution. from probaforms.models import RealNVP # fit nomalizing flow model model = RealNVP ( lr = 0.01 , n_epochs = 100 ) model . fit ( y , X ) # (target, condition) # sample new observations y_gen = model . sample ( X ) The figure below shows that Normalizing Flow (NF) successfully learnt the distribution p(y|X) . We can use NF to sample new objects from the distribution that are similar to the real observations. plt . figure ( figsize = ( 12 , 6 )) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y_gen , marker = '+' , label = 'Generated with NF' , color = 'C1' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , linewidth = 2 ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show () Now, we will repeat the sampling procedure several times and empirically estimate the mean value of the function and its standard deviation. The standard deviation we consider as the prediction uncertainty. The figure below demonstrates that NF successfully estimates the mean and standard deviation of the sample. y_preds = [] # repeat sampling several times for i in range ( 1000 ): # sample with NF y_gen = model . sample ( X ) y_preds . append ( y_gen ) y_preds = np . array ( y_preds ) # estimate the mean of the predictions mu_pred = y_preds . mean ( axis = 0 ) . reshape ( - 1 ,) # estimate the standard deviation of the predictions sigma_pred = y_preds . std ( axis = 0 ) . reshape ( - 1 ,) plt . figure ( figsize = ( 12 , 6 )) plt . fill_between ( X [:, 0 ], y1 = mu_pred + sigma_pred , y2 = mu_pred - sigma_pred , color = 'C1' , alpha = 0.5 , label = r '$\\mu \\pm \\sigma$ by NF' ) plt . plot ( X , mu_pred , label = 'Mean by NF' , color = 'C1' , linewidth = 4 ) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , color = 'C0' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show () Gaussian Process Finally, let\u2019s solve the same task with the Gaussian Process (GP) to compare the results. The plot below shows that GP predicts the mean with good precision. However, it is not able to learn the dependency of the standard deviation from the input variable X. from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import WhiteKernel , RBF , ConstantKernel as C # define kernel kernel = C () * RBF () + WhiteKernel () # fit GP gpr = GaussianProcessRegressor ( kernel = kernel ) gpr . fit ( X , y ) # make predictions of the means and standard deviations mu_pred_gp , sigma_pred_gp = gpr . predict ( X , return_std = True ) plt . figure ( figsize = ( 12 , 6 )) plt . fill_between ( X [:, 0 ], y1 = mu_pred_gp + sigma_pred_gp , y2 = mu_pred_gp - sigma_pred_gp , color = 'C1' , alpha = 0.5 , label = r '$\\mu \\pm \\sigma$ by GP' ) plt . plot ( X , mu_pred_gp , label = 'Mean by GP' , color = 'C1' , linewidth = 4 ) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , color = 'C0' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show ()","title":"Regression"},{"location":"examples/regression/#regression","text":"","title":"Regression"},{"location":"examples/regression/#generate-data-set","text":"In this example, we will solve a regression task with uncertainty estimation using Conditional Normalizing Flows. In addition, we will compare this approach with the Gaussian Process. Let\u2019s generate a data set with observations sampled from a normal distribution, where mean and variance depend on input variable X. # import of basic libraries import matplotlib.pyplot as plt from matplotlib import cm import pandas as pd import numpy as np # function we would like to predict def func ( X ): return np . exp ( - X ) # input variable X = np . linspace ( 0 , 5 , 500 ) . reshape ( - 1 , 1 ) # mean values of targets y mu = func ( X ) # normal noise eps = np . random . normal ( 0 , 1 , X . shape ) sigma = 0.05 * ( X + 0.5 ) # target variable we need to predict y = mu + eps * sigma plt . figure ( figsize = ( 12 , 6 )) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show ()","title":"Generate data set"},{"location":"examples/regression/#normalizing-flows","text":"Now, we use Conditional Real NVP to learn conditional distribution p(y|X) of the observations. Here, X is condition, y is our target to learn its distribution. from probaforms.models import RealNVP # fit nomalizing flow model model = RealNVP ( lr = 0.01 , n_epochs = 100 ) model . fit ( y , X ) # (target, condition) # sample new observations y_gen = model . sample ( X ) The figure below shows that Normalizing Flow (NF) successfully learnt the distribution p(y|X) . We can use NF to sample new objects from the distribution that are similar to the real observations. plt . figure ( figsize = ( 12 , 6 )) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y_gen , marker = '+' , label = 'Generated with NF' , color = 'C1' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , linewidth = 2 ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show () Now, we will repeat the sampling procedure several times and empirically estimate the mean value of the function and its standard deviation. The standard deviation we consider as the prediction uncertainty. The figure below demonstrates that NF successfully estimates the mean and standard deviation of the sample. y_preds = [] # repeat sampling several times for i in range ( 1000 ): # sample with NF y_gen = model . sample ( X ) y_preds . append ( y_gen ) y_preds = np . array ( y_preds ) # estimate the mean of the predictions mu_pred = y_preds . mean ( axis = 0 ) . reshape ( - 1 ,) # estimate the standard deviation of the predictions sigma_pred = y_preds . std ( axis = 0 ) . reshape ( - 1 ,) plt . figure ( figsize = ( 12 , 6 )) plt . fill_between ( X [:, 0 ], y1 = mu_pred + sigma_pred , y2 = mu_pred - sigma_pred , color = 'C1' , alpha = 0.5 , label = r '$\\mu \\pm \\sigma$ by NF' ) plt . plot ( X , mu_pred , label = 'Mean by NF' , color = 'C1' , linewidth = 4 ) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , color = 'C0' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show ()","title":"Normalizing flows"},{"location":"examples/regression/#gaussian-process","text":"Finally, let\u2019s solve the same task with the Gaussian Process (GP) to compare the results. The plot below shows that GP predicts the mean with good precision. However, it is not able to learn the dependency of the standard deviation from the input variable X. from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import WhiteKernel , RBF , ConstantKernel as C # define kernel kernel = C () * RBF () + WhiteKernel () # fit GP gpr = GaussianProcessRegressor ( kernel = kernel ) gpr . fit ( X , y ) # make predictions of the means and standard deviations mu_pred_gp , sigma_pred_gp = gpr . predict ( X , return_std = True ) plt . figure ( figsize = ( 12 , 6 )) plt . fill_between ( X [:, 0 ], y1 = mu_pred_gp + sigma_pred_gp , y2 = mu_pred_gp - sigma_pred_gp , color = 'C1' , alpha = 0.5 , label = r '$\\mu \\pm \\sigma$ by GP' ) plt . plot ( X , mu_pred_gp , label = 'Mean by GP' , color = 'C1' , linewidth = 4 ) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , color = 'C0' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show ()","title":"Gaussian Process"}]}