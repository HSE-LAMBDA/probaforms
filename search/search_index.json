{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to probaforms Probaforms is a python library of conditional Generative Adversarial Networks, Normalizing Flows, Variational Autoencoders and other generative models for tabular data. All models have a sklearn-like interface to enable rapid use in a variety of science and engineering applications. Installation git clone https : // github . com / HSE - LAMBDA / probaforms . git cd probaforms pip install - e . or poetry install Basic usage (See more examples in the documentation .) The following code snippet generates a noisy synthetic data, fits a conditional generative model, sample new objects, and displays the results. from sklearn.datasets import make_moons import matplotlib.pyplot as plt from probaforms.models import RealNVP # generate sample X with conditions C X , y = make_moons ( n_samples = 1000 , noise = 0.1 ) C = y . reshape ( - 1 , 1 ) # fit nomalizing flow model model = RealNVP ( lr = 0.01 , n_epochs = 100 ) model . fit ( X , C ) # sample new objects X_gen = model . sample ( C ) # display the results plt . scatter ( X_gen [ y == 0 , 0 ], X_gen [ y == 0 , 1 ]) plt . scatter ( X_gen [ y == 1 , 0 ], X_gen [ y == 1 , 1 ]) plt . show () Support Home: https://github.com/HSE-LAMBDA/probaforms Documentation: https://hse-lambda.github.io/probaforms For any usage questions, suggestions and bugs use the issue page , please. Thanks to all our contributors","title":"Home"},{"location":"#welcome-to-probaforms","text":"Probaforms is a python library of conditional Generative Adversarial Networks, Normalizing Flows, Variational Autoencoders and other generative models for tabular data. All models have a sklearn-like interface to enable rapid use in a variety of science and engineering applications.","title":"Welcome to probaforms"},{"location":"#installation","text":"git clone https : // github . com / HSE - LAMBDA / probaforms . git cd probaforms pip install - e . or poetry install","title":"Installation"},{"location":"#basic-usage","text":"(See more examples in the documentation .) The following code snippet generates a noisy synthetic data, fits a conditional generative model, sample new objects, and displays the results. from sklearn.datasets import make_moons import matplotlib.pyplot as plt from probaforms.models import RealNVP # generate sample X with conditions C X , y = make_moons ( n_samples = 1000 , noise = 0.1 ) C = y . reshape ( - 1 , 1 ) # fit nomalizing flow model model = RealNVP ( lr = 0.01 , n_epochs = 100 ) model . fit ( X , C ) # sample new objects X_gen = model . sample ( C ) # display the results plt . scatter ( X_gen [ y == 0 , 0 ], X_gen [ y == 0 , 1 ]) plt . scatter ( X_gen [ y == 1 , 0 ], X_gen [ y == 1 , 1 ]) plt . show ()","title":"Basic usage"},{"location":"#support","text":"Home: https://github.com/HSE-LAMBDA/probaforms Documentation: https://hse-lambda.github.io/probaforms For any usage questions, suggestions and bugs use the issue page , please.","title":"Support"},{"location":"#thanks-to-all-our-contributors","text":"","title":"Thanks to all our contributors"},{"location":"examples/Untitled/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); # import of basic libraries import matplotlib.pyplot as plt from matplotlib import cm import pandas as pd import numpy as np import probaforms from probaforms.metrics import frechet_distance , maximum_mean_discrepancy , kolmogorov_smirnov_1d , cramer_von_mises_1d , roc_auc_score_1d , anderson_darling_1d from probaforms.metrics import kullback_leibler_1d , jensen_shannon_1d from probaforms.metrics import kullback_leibler_1d_kde , jensen_shannon_1d_kde N = 1000 X_real = np . random . multivariate_normal ([ 0 , 0 ], [[ 1 , 0.7 ], [ 0.7 , 1 ]], N ) X_fake = np . random . multivariate_normal ([ 0 , 0.1 ], [[ 1 , 0.7 ], [ 0.7 , 1 ]], N ) plt . scatter ( X_real [:, 0 ], X_real [:, 1 ]) plt . scatter ( X_fake [:, 0 ], X_fake [:, 1 ]) plt . show () # X_real = np.random.uniform(0, 2, (N, 2))#.reshape(-1, 1) # X_fake = np.random.uniform(2, 4, (N, 2))#.reshape(-1, 1) # X_real = np.random.normal(0, 1, 1*N).reshape(-1, 1) # X_fake = np.random.normal(1, 1, 10*N).reshape(-1, 1) # plt.hist(X_real, bins=100) # plt.hist(X_fake, bins=100) # plt.show() frechet_distance ( X_real , X_fake ) (0.013181966971982057, 0.007189378587489877) maximum_mean_discrepancy ( X_real , X_fake ) (0.0018034432785270017, 0.0009774633351036257) %% time kolmogorov_smirnov_1d ( X_real , X_fake ) CPU times: user 156 ms, sys: 349 ms, total: 505 ms Wall time: 94.7 ms (0.05169500000000001, 0.010144923607400895) %% time cramer_von_mises_1d ( X_real , X_fake ) CPU times: user 117 ms, sys: 257 ms, total: 373 ms Wall time: 73.4 ms (0.3036530349999811, 0.17356490141291958) %% time anderson_darling_1d ( X_real , X_fake ) CPU times: user 170 ms, sys: 267 ms, total: 436 ms Wall time: 121 ms (0.8314070682204191, 0.9449794727511458) %% time roc_auc_score_1d ( X_real , X_fake ) CPU times: user 184 ms, sys: 1.53 ms, total: 185 ms Wall time: 185 ms (0.51486465, 0.006217353362766193) %% time kullback_leibler_1d ( X_real , X_fake , bins = 10 ) CPU times: user 56.6 ms, sys: 936 \u00b5s, total: 57.6 ms Wall time: 58.3 ms (0.031703702395971936, 0.01280640090355048) jensen_shannon_1d ( X_real , X_fake , bins = 10 ) (0.0044684660882367755, 0.0012118941991319965) %% time kullback_leibler_1d_kde ( X_real , X_fake ) CPU times: user 2.23 s, sys: 10.4 ms, total: 2.24 s Wall time: 2.24 s (0.011177978627755237, 0.004562473333272194) jensen_shannon_1d_kde ( X_real , X_fake ) (0.0026475359706036273, 0.0007658134445861451) mus = [] sis = [] bins = np . arange ( 5 , 100 , 1 ) for b in bins : mu , si = kullback_leibler_1d ( X_real , X_fake , bins = b ) mus . append ( mu ) sis . append ( si ) mus = np . array ( mus ) sis = np . array ( sis ) plt . plot ( bins , mus ) plt . show () mus = [] bins = np . arange ( 5 , 100 , 1 ) for b in bins : mu , _ = jensen_shannon_1d ( X_real , X_fake , bins = b ) mus . append ( mu ) plt . plot ( bins , mus ) plt . show () np . log ( 2 ) 0.6931471805599453 from sklearn.neighbors import KernelDensity %% time kd = KernelDensity ( bandwidth = 'silverman' ) x = X_fake [:, [ 0 ]] #x = np.concatenate((X_fake[:, [0]], X_real[:, [0]])) kd . fit ( x ) CPU times: user 1.02 ms, sys: 30 \u00b5s, total: 1.05 ms Wall time: 1.03 ms #sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\u25b8\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\u25be\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;} KernelDensity(bandwidth='silverman') In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. KernelDensity KernelDensity(bandwidth='silverman') %% time #a = np.linspace(x.min(), x.max(), 10).reshape(-1, 1) a = np . linspace ( - 5 , 10 , 101 ) . reshape ( - 1 , 1 ) p = np . exp ( kd . score_samples ( a )) p /= p . sum () CPU times: user 14 ms, sys: 251 \u00b5s, total: 14.2 ms Wall time: 14.1 ms p array([6.65811824e-154, 1.60520146e-147, 2.81763138e-141, 3.60101508e-135, 3.35090304e-129, 2.27041615e-123, 1.12012587e-117, 4.02399644e-112, 1.05266463e-106, 2.00528970e-101, 2.78184272e-096, 2.81041798e-091, 2.06779557e-086, 1.10804932e-081, 4.32457071e-077, 1.22935762e-072, 2.54557083e-068, 3.83959957e-064, 4.21895158e-060, 3.37728230e-056, 1.96971531e-052, 8.37037811e-049, 2.59195977e-045, 5.84916114e-042, 9.62026697e-039, 1.15335114e-035, 1.00803963e-032, 6.42400547e-030, 2.98558329e-027, 1.01215088e-024, 2.50363253e-022, 4.52006908e-020, 5.95851604e-018, 5.73794657e-016, 4.03882275e-014, 2.07945267e-012, 7.83853833e-011, 2.16577485e-009, 4.39260698e-008, 6.55217874e-007, 7.20571492e-006, 5.86155153e-005, 3.54229492e-004, 1.59977557e-003, 5.44341865e-003, 1.41152811e-002, 2.83550890e-002, 4.51926573e-002, 5.91704903e-002, 6.68062719e-002, 6.90485380e-002, 6.91611300e-002, 6.94925332e-002, 7.06206953e-002, 7.23650267e-002, 7.44339713e-002, 7.59521937e-002, 7.49640275e-002, 6.89836301e-002, 5.66465557e-002, 3.96348352e-002, 2.26523092e-002, 1.02255102e-002, 3.55893478e-003, 9.39547990e-004, 1.86106225e-004, 2.74588310e-005, 3.00270169e-006, 2.42502551e-007, 1.44268939e-008, 6.31010424e-010, 2.02606097e-011, 4.76981008e-013, 8.22548464e-015, 1.03821507e-016, 9.58494143e-019, 6.46880169e-021, 3.18994000e-023, 1.14891249e-025, 3.02122337e-028, 5.79877219e-031, 8.12137279e-034, 8.29774851e-037, 6.18355183e-040, 3.36033556e-043, 1.33144497e-046, 3.84588736e-050, 8.09741267e-054, 1.24257283e-057, 1.38956205e-061, 1.13233173e-065, 6.72313974e-070, 2.90830457e-074, 9.16529701e-079, 2.10409062e-083, 3.51858714e-088, 4.28583083e-093, 3.80227348e-098, 2.45682237e-103, 1.15613417e-108, 3.96214299e-114]) plt . plot ( a , p ) plt . show () plt . hist ( x , bins = 100 ) plt . show () from probaforms.metrics import maximum_mean_discrepancy from probaforms.models.interfaces import GenModel def subclasses ( cls ): return set ( cls . __subclasses__ ()) . union ( s for c in cls . __subclasses__ () for s in subclasses ( c )) subclasses ( GenModel ) {probaforms.models.realnvp.RealNVP} from probaforms import metrics dir ( metrics ) for f in metrics . __all__ : f . --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In[223], line 5 3 dir (metrics) 4 for f in metrics . __all__: ----> 5 f ( ) TypeError : 'str' object is not callable from inspect import getmembers , isfunction for f in getmembers ( metrics , isfunction ): print ( f [ 1 ]( X_real , X_fake )) (0.1551333509553631, 0.08726909764210516) (0.055396212963567296, 0.014707551332771785) (0.1542, 0.0269325082381868) (0.5726464268961485, 0.25541443717036655) (0.01849286009372232, 0.009703837639500987) funcs = [ i [ 1 ] for i in getmembers ( metrics , isfunction )] funcs [<function probaforms.metrics.fd.frechet_distance(X_real, X_fake, n_iters=100, standardize=False)>, <function probaforms.metrics.div1d.jensen_shannon_1d(X_real, X_fake, n_iters=100, bins=10)>, <function probaforms.metrics.ks1d.kolmogorov_smirnov_1d(X_real, X_fake, n_iters=100)>, <function probaforms.metrics.div1d.kullback_leibler_1d(X_real, X_fake, n_iters=100, bins=10)>, <function probaforms.metrics.mmd.maximum_mean_discrepancy(X, Y, n_iters=100, standardize=True)>] np . histogram ( X_real ) (array([ 3, 20, 105, 271, 498, 559, 381, 126, 28, 9]), array([-3.71608916, -2.99241292, -2.26873667, -1.54506042, -0.82138417, -0.09770792, 0.62596833, 1.34964458, 2.07332083, 2.79699708, 3.52067333])) def compute_probs1d ( data , bins = 10 ): h , e = np . histogram ( data , bins ) p = h / h . sum () return p , e def get_probs1d ( data1 , data2 , bins = 10 ): data12 = np . concatenate (( data1 , data2 ), axis = 0 ) _ , e = compute_probs1d ( data12 , bins ) p , _ = compute_probs1d ( data1 , e ) q , _ = compute_probs1d ( data2 , e ) return p , q def kl_divergence ( p , q ): return np . sum ( p * np . log ( p / q )) p , q = get_probs1d ( X_real , X_fake , 10 ) p , q (array([0.0015, 0.0215, 0.0915, 0.1955, 0.3 , 0.2325, 0.1245, 0.0295, 0.0035, 0. ]), array([0.0005, 0.01 , 0.0555, 0.1275, 0.2215, 0.265 , 0.187 , 0.0935, 0.0345, 0.005 ])) e = 10 **- 10 kl_divergence ( p + e , q + e ) 0.11531850969046133 p / q array([3. , 2.15 , 1.64864865, 1.53333333, 1.35440181, 0.87735849, 0.6657754 , 0.31550802, 0.10144928, 0. ]) p . sum () 1.0 for i in zip ([ 1 ], [ 3 ]): print ( i ) (1, 3) lambda x : ( x [ 0 ] != 0 ) & ( x [ 1 ] != 0 ), zip ([ 1 ], [ 3 ]) (<function __main__.<lambda>(x)>, <zip at 0x7fc0f817d8c0>) def support_intersection ( p , q ): sup_int = ( list ( filter ( lambda x : ( x [ 0 ] != 0 ) & ( x [ 1 ] != 0 ), zip ( p , q ) ) ) ) return sup_int support_intersection ([ 1 , 5 ], [ 3 , 3 ]) [(1, 3), (5, 3)] def a ( x ): return x a ( 1 ) 1 def b ( a , z , * args ): return a ( z , * args ) b ( a , 3 ) 3 0.1 * np . log ( 0.1 / 10 **- 100 ) 22.795592420641054 from sklearn.model_selection import KFold kf = KFold ( n_splits = 5 , shuffle = True ) for (( _ , test_ids1 ), ( _ , test_ids2 )) in zip ( kf . split ( X_real ), kf . split ( X_fake )): print ( test_ids1 . shape , test_ids2 . shape ) (20,) (200,) (20,) (200,) (20,) (200,) (20,) (200,) (20,) (200,) test_ids1 array([ 1, 2, 19, 24, 25, 29, 31, 37, 38, 41, 44, 46, 55, 56, 59, 72, 75, 76, 83, 92]) test_ids2 array([ 0, 1, 7, 9, 10, 11, 20, 24, 33, 35, 43, 44, 47, 48, 50, 53, 54, 55, 61, 65, 66, 73, 81, 83, 87, 89, 95, 96, 98, 111, 116, 122, 137, 139, 143, 157, 158, 166, 167, 179, 186, 188, 194, 207, 218, 219, 224, 226, 228, 231, 236, 237, 238, 245, 249, 250, 252, 266, 269, 272, 280, 295, 297, 308, 312, 317, 320, 322, 324, 333, 334, 339, 343, 347, 349, 351, 356, 361, 369, 376, 379, 384, 385, 398, 406, 413, 414, 423, 426, 428, 431, 433, 436, 437, 447, 451, 460, 466, 470, 475, 476, 484, 488, 500, 502, 504, 506, 511, 514, 524, 525, 531, 532, 534, 539, 541, 542, 550, 552, 564, 570, 574, 593, 594, 603, 610, 615, 630, 632, 638, 641, 642, 643, 644, 651, 652, 657, 660, 663, 665, 666, 671, 673, 679, 680, 683, 687, 692, 698, 715, 716, 727, 732, 733, 743, 750, 753, 755, 767, 768, 771, 776, 780, 781, 789, 791, 794, 800, 806, 811, 815, 829, 840, 842, 844, 852, 855, 868, 878, 885, 886, 888, 895, 906, 914, 915, 916, 917, 921, 924, 928, 937, 945, 957, 967, 970, 983, 985, 990, 991])","title":"Untitled"},{"location":"examples/Untitled1/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }());","title":"Untitled1"},{"location":"examples/metrics/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Metrics usage This example shows how to use metrics to measure the quality of the generated samples. The metrics take two samples (real and generated) and estimate discrepancies between their distributions. The smaller metrics values the better. Generate two samples Let's generate two samples from multivariate normal distributions with the same covariance matrices, but with some distance between their means. import matplotlib.pyplot as plt import numpy as np from probaforms import metrics def gen_two_samples ( dist , N ): sigma = np . array ([[ 1 , 0.7 ], [ 0.7 , 1 ]]) mu_x = np . array ([ 0 , 0 ]) mu_y = mu_x + dist / np . sqrt ( 2 ) X = np . random . multivariate_normal ( mu_x , sigma , N ) Y = np . random . multivariate_normal ( mu_y , sigma , N ) return X , Y # generate two samples with a size of 1000 dist = 2 X , Y = gen_two_samples ( dist , N = 1000 ) def plot_samples ( X , Y , dist = 0 ): plt . figure ( figsize = ( 6 , 4 )) plt . scatter ( X [:, 0 ], X [:, 1 ], label = 'X' , alpha = 0.5 , color = 'C0' ) plt . scatter ( Y [:, 0 ], Y [:, 1 ], label = 'Y' , alpha = 0.5 , color = 'C1' ) plt . title ( \"Distance = %.f\" % ( dist )) plt . legend () plt . tight_layout () plt . show () plot_samples ( X , Y , dist ) Compute discrepancies between the samples def get_metrics ( X , Y ): mu , sigma = metrics . frechet_distance ( X , Y ) print ( r \"Frechet Distance = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . kolmogorov_smirnov_1d ( X , Y ) print ( r \"Kolmogorov-Smirnov = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . cramer_von_mises_1d ( X , Y ) print ( r \"Cramer-von Mises = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . anderson_darling_1d ( X , Y ) print ( r \"Anderson-Darling = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . roc_auc_score_1d ( X , Y ) print ( r \"ROC AUC = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . kullback_leibler_1d_kde ( X , Y ) print ( r \"Kullback-Leibler KDE = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . jensen_shannon_1d_kde ( X , Y ) print ( r \"Jensen-Shannon KDE = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . maximum_mean_discrepancy ( X , Y ) print ( r \"Maximum Mean Discrepancy = %.6f +- %.6f \" % ( mu , sigma )) get_metrics ( X , Y ) Frechet Distance = 3.711472 +- 0.240218 Kolmogorov-Smirnov = 0.509375 +- 0.015197 Cramer-von Mises = 64.544793 +- 3.777022 Anderson-Darling = 423.161173 +- 20.731482 ROC AUC = 0.824993 +- 0.007886 Kullback-Leibler KDE = 0.852085 +- 0.063555 Jensen-Shannon KDE = 0.173695 +- 0.009267 Maximum Mean Discrepancy = 0.305732 +- 0.019054 Additional experiments with other distances dist = 10. X , Y = gen_two_samples ( dist , N = 1000 ) plot_samples ( X , Y , dist ) get_metrics ( X , Y ) Frechet Distance = 100.640957 +- 1.078155 Kolmogorov-Smirnov = 1.000000 +- 0.000000 Cramer-von Mises = 166.667082 +- 0.000017 Anderson-Darling = 1015.172596 +- 0.022680 ROC AUC = 1.000000 +- 0.000000 Kullback-Leibler KDE = 12.633292 +- 0.031014 Jensen-Shannon KDE = 0.692143 +- 0.000470 Maximum Mean Discrepancy = 1.502160 +- 0.007987 dist = 0. X , Y = gen_two_samples ( dist , N = 1000 ) plot_samples ( X , Y , dist ) get_metrics ( X , Y ) Frechet Distance = 0.007647 +- 0.005009 Kolmogorov-Smirnov = 0.053815 +- 0.010366 Cramer-von Mises = 0.325126 +- 0.186177 Anderson-Darling = 0.955208 +- 0.973257 ROC AUC = 0.513250 +- 0.005923 Kullback-Leibler KDE = 0.011113 +- 0.003471 Jensen-Shannon KDE = 0.002718 +- 0.000755 Maximum Mean Discrepancy = 0.001662 +- 0.001076","title":"Metrics usage"},{"location":"examples/metrics/#metrics-usage","text":"This example shows how to use metrics to measure the quality of the generated samples. The metrics take two samples (real and generated) and estimate discrepancies between their distributions. The smaller metrics values the better.","title":"Metrics usage"},{"location":"examples/metrics/#generate-two-samples","text":"Let's generate two samples from multivariate normal distributions with the same covariance matrices, but with some distance between their means. import matplotlib.pyplot as plt import numpy as np from probaforms import metrics def gen_two_samples ( dist , N ): sigma = np . array ([[ 1 , 0.7 ], [ 0.7 , 1 ]]) mu_x = np . array ([ 0 , 0 ]) mu_y = mu_x + dist / np . sqrt ( 2 ) X = np . random . multivariate_normal ( mu_x , sigma , N ) Y = np . random . multivariate_normal ( mu_y , sigma , N ) return X , Y # generate two samples with a size of 1000 dist = 2 X , Y = gen_two_samples ( dist , N = 1000 ) def plot_samples ( X , Y , dist = 0 ): plt . figure ( figsize = ( 6 , 4 )) plt . scatter ( X [:, 0 ], X [:, 1 ], label = 'X' , alpha = 0.5 , color = 'C0' ) plt . scatter ( Y [:, 0 ], Y [:, 1 ], label = 'Y' , alpha = 0.5 , color = 'C1' ) plt . title ( \"Distance = %.f\" % ( dist )) plt . legend () plt . tight_layout () plt . show () plot_samples ( X , Y , dist )","title":"Generate two samples"},{"location":"examples/metrics/#compute-discrepancies-between-the-samples","text":"def get_metrics ( X , Y ): mu , sigma = metrics . frechet_distance ( X , Y ) print ( r \"Frechet Distance = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . kolmogorov_smirnov_1d ( X , Y ) print ( r \"Kolmogorov-Smirnov = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . cramer_von_mises_1d ( X , Y ) print ( r \"Cramer-von Mises = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . anderson_darling_1d ( X , Y ) print ( r \"Anderson-Darling = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . roc_auc_score_1d ( X , Y ) print ( r \"ROC AUC = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . kullback_leibler_1d_kde ( X , Y ) print ( r \"Kullback-Leibler KDE = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . jensen_shannon_1d_kde ( X , Y ) print ( r \"Jensen-Shannon KDE = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . maximum_mean_discrepancy ( X , Y ) print ( r \"Maximum Mean Discrepancy = %.6f +- %.6f \" % ( mu , sigma )) get_metrics ( X , Y ) Frechet Distance = 3.711472 +- 0.240218 Kolmogorov-Smirnov = 0.509375 +- 0.015197 Cramer-von Mises = 64.544793 +- 3.777022 Anderson-Darling = 423.161173 +- 20.731482 ROC AUC = 0.824993 +- 0.007886 Kullback-Leibler KDE = 0.852085 +- 0.063555 Jensen-Shannon KDE = 0.173695 +- 0.009267 Maximum Mean Discrepancy = 0.305732 +- 0.019054","title":"Compute discrepancies between the samples"},{"location":"examples/metrics/#additional-experiments-with-other-distances","text":"dist = 10. X , Y = gen_two_samples ( dist , N = 1000 ) plot_samples ( X , Y , dist ) get_metrics ( X , Y ) Frechet Distance = 100.640957 +- 1.078155 Kolmogorov-Smirnov = 1.000000 +- 0.000000 Cramer-von Mises = 166.667082 +- 0.000017 Anderson-Darling = 1015.172596 +- 0.022680 ROC AUC = 1.000000 +- 0.000000 Kullback-Leibler KDE = 12.633292 +- 0.031014 Jensen-Shannon KDE = 0.692143 +- 0.000470 Maximum Mean Discrepancy = 1.502160 +- 0.007987 dist = 0. X , Y = gen_two_samples ( dist , N = 1000 ) plot_samples ( X , Y , dist ) get_metrics ( X , Y ) Frechet Distance = 0.007647 +- 0.005009 Kolmogorov-Smirnov = 0.053815 +- 0.010366 Cramer-von Mises = 0.325126 +- 0.186177 Anderson-Darling = 0.955208 +- 0.973257 ROC AUC = 0.513250 +- 0.005923 Kullback-Leibler KDE = 0.011113 +- 0.003471 Jensen-Shannon KDE = 0.002718 +- 0.000755 Maximum Mean Discrepancy = 0.001662 +- 0.001076","title":"Additional experiments with other distances"},{"location":"examples/regression/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Regression Generate data set In this example, we will solve a regression task with uncertainty estimation using Conditional Normalizing Flows. In addition, we will compare this approach with the Gaussian Process. Let\u2019s generate a data set with observations sampled from a normal distribution, where mean and variance depend on input variable X. # import of basic libraries import matplotlib.pyplot as plt from matplotlib import cm import pandas as pd import numpy as np # function we would like to predict def func ( X ): return np . exp ( - X ) # input variable X = np . linspace ( 0 , 5 , 500 ) . reshape ( - 1 , 1 ) # mean values of targets y mu = func ( X ) # normal noise eps = np . random . normal ( 0 , 1 , X . shape ) sigma = 0.05 * ( X + 0.5 ) # target variable we need to predict y = mu + eps * sigma plt . figure ( figsize = ( 12 , 6 )) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show () Normalizing flows Now, we use Conditional Real NVP to learn conditional distribution p(y|X) of the observations. Here, X is condition, y is our target to learn its distribution. from probaforms.models import RealNVP # fit nomalizing flow model model = RealNVP ( lr = 0.01 , n_epochs = 100 ) model . fit ( y , X ) # (target, condition) # sample new observations y_gen = model . sample ( X ) The figure below shows that Normalizing Flow (NF) successfully learnt the distribution p(y|X) . We can use NF to sample new objects from the distribution that are similar to the real observations. plt . figure ( figsize = ( 12 , 6 )) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y_gen , marker = '+' , label = 'Generated with NF' , color = 'C1' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , linewidth = 2 ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show () Now, we will repeat the sampling procedure several times and empirically estimate the mean value of the function and its standard deviation. The standard deviation we consider as the prediction uncertainty. The figure below demonstrates that NF successfully estimates the mean and standard deviation of the sample. y_preds = [] # repeat sampling several times for i in range ( 1000 ): # sample with NF y_gen = model . sample ( X ) y_preds . append ( y_gen ) y_preds = np . array ( y_preds ) # estimate the mean of the predictions mu_pred = y_preds . mean ( axis = 0 ) . reshape ( - 1 ,) # estimate the standard deviation of the predictions sigma_pred = y_preds . std ( axis = 0 ) . reshape ( - 1 ,) plt . figure ( figsize = ( 12 , 6 )) plt . fill_between ( X [:, 0 ], y1 = mu_pred + sigma_pred , y2 = mu_pred - sigma_pred , color = 'C1' , alpha = 0.5 , label = r '$\\mu \\pm \\sigma$ by NF' ) plt . plot ( X , mu_pred , label = 'Mean by NF' , color = 'C1' , linewidth = 4 ) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , color = 'C0' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show () Gaussian Process Finally, let\u2019s solve the same task with the Gaussian Process (GP) to compare the results. The plot below shows that GP predicts the mean with good precision. However, it is not able to learn the dependency of the standard deviation from the input variable X. from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import WhiteKernel , RBF , ConstantKernel as C # define kernel kernel = C () * RBF () + WhiteKernel () # fit GP gpr = GaussianProcessRegressor ( kernel = kernel ) gpr . fit ( X , y ) # make predictions of the means and standard deviations mu_pred_gp , sigma_pred_gp = gpr . predict ( X , return_std = True ) plt . figure ( figsize = ( 12 , 6 )) plt . fill_between ( X [:, 0 ], y1 = mu_pred_gp + sigma_pred_gp , y2 = mu_pred_gp - sigma_pred_gp , color = 'C1' , alpha = 0.5 , label = r '$\\mu \\pm \\sigma$ by GP' ) plt . plot ( X , mu_pred_gp , label = 'Mean by GP' , color = 'C1' , linewidth = 4 ) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , color = 'C0' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show ()","title":"Regression"},{"location":"examples/regression/#regression","text":"","title":"Regression"},{"location":"examples/regression/#generate-data-set","text":"In this example, we will solve a regression task with uncertainty estimation using Conditional Normalizing Flows. In addition, we will compare this approach with the Gaussian Process. Let\u2019s generate a data set with observations sampled from a normal distribution, where mean and variance depend on input variable X. # import of basic libraries import matplotlib.pyplot as plt from matplotlib import cm import pandas as pd import numpy as np # function we would like to predict def func ( X ): return np . exp ( - X ) # input variable X = np . linspace ( 0 , 5 , 500 ) . reshape ( - 1 , 1 ) # mean values of targets y mu = func ( X ) # normal noise eps = np . random . normal ( 0 , 1 , X . shape ) sigma = 0.05 * ( X + 0.5 ) # target variable we need to predict y = mu + eps * sigma plt . figure ( figsize = ( 12 , 6 )) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show ()","title":"Generate data set"},{"location":"examples/regression/#normalizing-flows","text":"Now, we use Conditional Real NVP to learn conditional distribution p(y|X) of the observations. Here, X is condition, y is our target to learn its distribution. from probaforms.models import RealNVP # fit nomalizing flow model model = RealNVP ( lr = 0.01 , n_epochs = 100 ) model . fit ( y , X ) # (target, condition) # sample new observations y_gen = model . sample ( X ) The figure below shows that Normalizing Flow (NF) successfully learnt the distribution p(y|X) . We can use NF to sample new objects from the distribution that are similar to the real observations. plt . figure ( figsize = ( 12 , 6 )) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y_gen , marker = '+' , label = 'Generated with NF' , color = 'C1' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , linewidth = 2 ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show () Now, we will repeat the sampling procedure several times and empirically estimate the mean value of the function and its standard deviation. The standard deviation we consider as the prediction uncertainty. The figure below demonstrates that NF successfully estimates the mean and standard deviation of the sample. y_preds = [] # repeat sampling several times for i in range ( 1000 ): # sample with NF y_gen = model . sample ( X ) y_preds . append ( y_gen ) y_preds = np . array ( y_preds ) # estimate the mean of the predictions mu_pred = y_preds . mean ( axis = 0 ) . reshape ( - 1 ,) # estimate the standard deviation of the predictions sigma_pred = y_preds . std ( axis = 0 ) . reshape ( - 1 ,) plt . figure ( figsize = ( 12 , 6 )) plt . fill_between ( X [:, 0 ], y1 = mu_pred + sigma_pred , y2 = mu_pred - sigma_pred , color = 'C1' , alpha = 0.5 , label = r '$\\mu \\pm \\sigma$ by NF' ) plt . plot ( X , mu_pred , label = 'Mean by NF' , color = 'C1' , linewidth = 4 ) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , color = 'C0' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show ()","title":"Normalizing flows"},{"location":"examples/regression/#gaussian-process","text":"Finally, let\u2019s solve the same task with the Gaussian Process (GP) to compare the results. The plot below shows that GP predicts the mean with good precision. However, it is not able to learn the dependency of the standard deviation from the input variable X. from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import WhiteKernel , RBF , ConstantKernel as C # define kernel kernel = C () * RBF () + WhiteKernel () # fit GP gpr = GaussianProcessRegressor ( kernel = kernel ) gpr . fit ( X , y ) # make predictions of the means and standard deviations mu_pred_gp , sigma_pred_gp = gpr . predict ( X , return_std = True ) plt . figure ( figsize = ( 12 , 6 )) plt . fill_between ( X [:, 0 ], y1 = mu_pred_gp + sigma_pred_gp , y2 = mu_pred_gp - sigma_pred_gp , color = 'C1' , alpha = 0.5 , label = r '$\\mu \\pm \\sigma$ by GP' ) plt . plot ( X , mu_pred_gp , label = 'Mean by GP' , color = 'C1' , linewidth = 4 ) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , color = 'C0' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show ()","title":"Gaussian Process"}]}