{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to probaforms Probaforms is a python library of conditional Generative Adversarial Networks, Normalizing Flows, Variational Autoencoders and other generative models for tabular data. All models have a sklearn-like interface to enable rapid use in a variety of science and engineering applications. Implemented conditional models Variational Autoencoder (CVAE) Wasserstein GAN (WGAN) Real NVP Installation pip install probaforms or git clone https : // github . com / HSE - LAMBDA / probaforms . git cd probaforms pip install - e . or poetry install Basic usage (See more examples in the documentation .) The following code snippet generates a noisy synthetic data, fits a conditional generative model, sample new objects, and displays the results. from sklearn.datasets import make_moons import matplotlib.pyplot as plt from probaforms.models import RealNVP # generate sample X with conditions C X , y = make_moons ( n_samples = 1000 , noise = 0.1 ) C = y . reshape ( - 1 , 1 ) # fit nomalizing flow model model = RealNVP ( lr = 0.01 , n_epochs = 100 ) model . fit ( X , C ) # sample new objects X_gen = model . sample ( C ) # display the results plt . scatter ( X_gen [ y == 0 , 0 ], X_gen [ y == 0 , 1 ]) plt . scatter ( X_gen [ y == 1 , 0 ], X_gen [ y == 1 , 1 ]) plt . show () Support Home: https://github.com/HSE-LAMBDA/probaforms Documentation: https://hse-lambda.github.io/probaforms For any usage questions, suggestions and bugs use the issue page , please. Thanks to all our contributors","title":"Home"},{"location":"#welcome-to-probaforms","text":"Probaforms is a python library of conditional Generative Adversarial Networks, Normalizing Flows, Variational Autoencoders and other generative models for tabular data. All models have a sklearn-like interface to enable rapid use in a variety of science and engineering applications.","title":"Welcome to probaforms"},{"location":"#implemented-conditional-models","text":"Variational Autoencoder (CVAE) Wasserstein GAN (WGAN) Real NVP","title":"Implemented conditional models"},{"location":"#installation","text":"pip install probaforms or git clone https : // github . com / HSE - LAMBDA / probaforms . git cd probaforms pip install - e . or poetry install","title":"Installation"},{"location":"#basic-usage","text":"(See more examples in the documentation .) The following code snippet generates a noisy synthetic data, fits a conditional generative model, sample new objects, and displays the results. from sklearn.datasets import make_moons import matplotlib.pyplot as plt from probaforms.models import RealNVP # generate sample X with conditions C X , y = make_moons ( n_samples = 1000 , noise = 0.1 ) C = y . reshape ( - 1 , 1 ) # fit nomalizing flow model model = RealNVP ( lr = 0.01 , n_epochs = 100 ) model . fit ( X , C ) # sample new objects X_gen = model . sample ( C ) # display the results plt . scatter ( X_gen [ y == 0 , 0 ], X_gen [ y == 0 , 1 ]) plt . scatter ( X_gen [ y == 1 , 0 ], X_gen [ y == 1 , 1 ]) plt . show ()","title":"Basic usage"},{"location":"#support","text":"Home: https://github.com/HSE-LAMBDA/probaforms Documentation: https://hse-lambda.github.io/probaforms For any usage questions, suggestions and bugs use the issue page , please.","title":"Support"},{"location":"#thanks-to-all-our-contributors","text":"","title":"Thanks to all our contributors"},{"location":"examples/forecast/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Probabilistic Time Series Forecast In this example, we will consider how to use conditional generative models for probabilistic time series forecast. The generative models help to estimate confidence intervals of the predictions. import matplotlib.pyplot as plt import pandas as pd import numpy as np Read data set We will predict the number of international airline passengers in units of 1,000. The data ranges from January 1949 to December 1960, or 12 years, with 144 observations. # data download ! wget https : // raw . githubusercontent . com / jbrownlee / Datasets / master / airline - passengers . csv # read the data data = pd . read_csv ( \"airline-passengers.csv\" ) data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Month Passengers 0 1949-01 112 1 1949-02 118 2 1949-03 132 3 1949-04 129 4 1949-05 121 Data visualization # get a time series Y = data [[ 'Passengers' ]] . values plt . figure ( figsize = ( 12 , 4 )) plt . plot ( Y , label = 'True' ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . tight_layout () plt . show () Preprocessing # scaling from sklearn.preprocessing import StandardScaler Y = StandardScaler () . fit_transform ( Y ) plt . figure ( figsize = ( 12 , 4 )) plt . plot ( Y , label = 'True' ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . tight_layout () plt . show () Autoregression model for time series forecast Consider y_1, y_2, ..., y_i, ..., y_N are observations of a time series. Autoegression model (AR) for the time series forecast assumes the following: \\hat{y}_{i+m} = f( y_{i}, y_{i-1}, ... y_{i-k+1} ) where \\hat{y}_{i+m} is a predicted value. In matrix form we will define this model as: \\hat{Y} = f(X) where X = \\left( \\begin{array}{cccc} y_{k} & y_{k-1} & \\ldots & y_{1}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ y_{k+j} & y_{k+j-1} & \\ldots & y_{j+1}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\end{array} \\right) Y = \\left( \\begin{array}{c} y_{k+m} \\\\ \\vdots\\\\ y_{k+j+m}\\\\ \\vdots \\end{array} \\right) K = 10 M = 10 def AR_matrices ( Y , K , M ): X_AR = [] Y_AR = [] for i in range ( len ( Y )): if i < K - 1 : continue if i + M >= len ( Y ): break ax_ar = Y [ i + 1 - K : i + 1 ] . reshape ( - 1 , ) X_AR . append ( ax_ar ) ay_ar = Y [ i + M ] #[0] # predict only y_{i+M} # ay_ar = Y[i+1:i+M+1].reshape(-1, ) # predict y_{i+M}, ..., y_{i+2}, y_{i+1} Y_AR . append ( ay_ar ) return np . array ( X_AR ), np . array ( Y_AR ) # prepare X and Y matrices X_AR , Y_AR = AR_matrices ( Y , K , M ) X_AR [: 2 ] array([[-1.40777884, -1.35759023, -1.24048348, -1.26557778, -1.33249593, -1.21538918, -1.10664719, -1.10664719, -1.20702441, -1.34922546], [-1.35759023, -1.24048348, -1.26557778, -1.33249593, -1.21538918, -1.10664719, -1.10664719, -1.20702441, -1.34922546, -1.47469699]]) Y_AR [: 2 ] array([[-0.9226223 ], [-1.02299951]]) X_AR . shape , Y_AR . shape ((125, 10), (125, 1)) Train and test split N = 90 X_AR_train , X_AR_test = X_AR [: N ], X_AR [ N :] Y_AR_train , Y_AR_test = Y_AR [: N ], Y_AR [ N :] Fit probabilistic model from probaforms.models import RealNVP # fit nomalizing flow model model = RealNVP ( lr = 0.01 , n_epochs = 100 , weight_decay = 0.2 ) model . fit ( Y_AR_train , X_AR_train ) plt . figure ( figsize = ( 12 , 4 )) plt . plot ( model . loss_history ) plt . xlabel ( 'Number of iterations' ) plt . ylabel ( 'Loss' ) plt . tight_layout () plt . show () Single prediction # sample new objects Y_pred_test = model . sample ( X_AR_test ) plt . figure ( figsize = ( 12 , 4 )) plt . plot ( Y_AR_test [:, - 1 ], label = 'True' , alpha = 1. ) plt . plot ( Y_pred_test [:, - 1 ], label = 'Prediction' , alpha = 1. ) plt . legend ( loc = 'best' ) plt . tight_layout () plt . show () Multiple predictions predictions_test = [] for i in range ( 1000 ): Y_pred_test = model . sample ( X_AR_test ) # store predictions predictions_test . append ( Y_pred_test [:, - 1 ]) predictions_test = np . array ( predictions_test ) plt . figure ( figsize = ( 12 , 4 )) # mean prediction plt . plot ( predictions_test . mean ( axis = 0 ), label = 'Prediction (mean)' , alpha = 1. , color = '0' ) # 90% CI xx = np . arange ( len ( Y_pred_test )) y_top = np . quantile ( predictions_test , q = 0.95 , axis = 0 ) y_bot = np . quantile ( predictions_test , q = 0.05 , axis = 0 ) plt . fill_between ( xx , y_bot , y_top , label = \"90% CI\" , alpha = 0.5 , color = 'C0' ) # true plt . plot ( Y_AR_test [:, - 1 ], label = 'True' , alpha = 1. , color = 'C1' ) plt . legend ( loc = 'best' ) plt . tight_layout () plt . show ()","title":"Time Series Forecast"},{"location":"examples/forecast/#probabilistic-time-series-forecast","text":"In this example, we will consider how to use conditional generative models for probabilistic time series forecast. The generative models help to estimate confidence intervals of the predictions. import matplotlib.pyplot as plt import pandas as pd import numpy as np","title":"Probabilistic Time Series Forecast"},{"location":"examples/forecast/#read-data-set","text":"We will predict the number of international airline passengers in units of 1,000. The data ranges from January 1949 to December 1960, or 12 years, with 144 observations. # data download ! wget https : // raw . githubusercontent . com / jbrownlee / Datasets / master / airline - passengers . csv # read the data data = pd . read_csv ( \"airline-passengers.csv\" ) data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Month Passengers 0 1949-01 112 1 1949-02 118 2 1949-03 132 3 1949-04 129 4 1949-05 121","title":"Read data set"},{"location":"examples/forecast/#data-visualization","text":"# get a time series Y = data [[ 'Passengers' ]] . values plt . figure ( figsize = ( 12 , 4 )) plt . plot ( Y , label = 'True' ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . tight_layout () plt . show ()","title":"Data visualization"},{"location":"examples/forecast/#preprocessing","text":"# scaling from sklearn.preprocessing import StandardScaler Y = StandardScaler () . fit_transform ( Y ) plt . figure ( figsize = ( 12 , 4 )) plt . plot ( Y , label = 'True' ) plt . legend ( loc = 'best' , fontsize = 14 ) plt . tight_layout () plt . show ()","title":"Preprocessing"},{"location":"examples/forecast/#autoregression-model-for-time-series-forecast","text":"Consider y_1, y_2, ..., y_i, ..., y_N are observations of a time series. Autoegression model (AR) for the time series forecast assumes the following: \\hat{y}_{i+m} = f( y_{i}, y_{i-1}, ... y_{i-k+1} ) where \\hat{y}_{i+m} is a predicted value. In matrix form we will define this model as: \\hat{Y} = f(X) where X = \\left( \\begin{array}{cccc} y_{k} & y_{k-1} & \\ldots & y_{1}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ y_{k+j} & y_{k+j-1} & \\ldots & y_{j+1}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\end{array} \\right) Y = \\left( \\begin{array}{c} y_{k+m} \\\\ \\vdots\\\\ y_{k+j+m}\\\\ \\vdots \\end{array} \\right) K = 10 M = 10 def AR_matrices ( Y , K , M ): X_AR = [] Y_AR = [] for i in range ( len ( Y )): if i < K - 1 : continue if i + M >= len ( Y ): break ax_ar = Y [ i + 1 - K : i + 1 ] . reshape ( - 1 , ) X_AR . append ( ax_ar ) ay_ar = Y [ i + M ] #[0] # predict only y_{i+M} # ay_ar = Y[i+1:i+M+1].reshape(-1, ) # predict y_{i+M}, ..., y_{i+2}, y_{i+1} Y_AR . append ( ay_ar ) return np . array ( X_AR ), np . array ( Y_AR ) # prepare X and Y matrices X_AR , Y_AR = AR_matrices ( Y , K , M ) X_AR [: 2 ] array([[-1.40777884, -1.35759023, -1.24048348, -1.26557778, -1.33249593, -1.21538918, -1.10664719, -1.10664719, -1.20702441, -1.34922546], [-1.35759023, -1.24048348, -1.26557778, -1.33249593, -1.21538918, -1.10664719, -1.10664719, -1.20702441, -1.34922546, -1.47469699]]) Y_AR [: 2 ] array([[-0.9226223 ], [-1.02299951]]) X_AR . shape , Y_AR . shape ((125, 10), (125, 1))","title":"Autoregression model for time series forecast"},{"location":"examples/forecast/#train-and-test-split","text":"N = 90 X_AR_train , X_AR_test = X_AR [: N ], X_AR [ N :] Y_AR_train , Y_AR_test = Y_AR [: N ], Y_AR [ N :]","title":"Train and test split"},{"location":"examples/forecast/#fit-probabilistic-model","text":"from probaforms.models import RealNVP # fit nomalizing flow model model = RealNVP ( lr = 0.01 , n_epochs = 100 , weight_decay = 0.2 ) model . fit ( Y_AR_train , X_AR_train ) plt . figure ( figsize = ( 12 , 4 )) plt . plot ( model . loss_history ) plt . xlabel ( 'Number of iterations' ) plt . ylabel ( 'Loss' ) plt . tight_layout () plt . show ()","title":"Fit probabilistic model"},{"location":"examples/forecast/#single-prediction","text":"# sample new objects Y_pred_test = model . sample ( X_AR_test ) plt . figure ( figsize = ( 12 , 4 )) plt . plot ( Y_AR_test [:, - 1 ], label = 'True' , alpha = 1. ) plt . plot ( Y_pred_test [:, - 1 ], label = 'Prediction' , alpha = 1. ) plt . legend ( loc = 'best' ) plt . tight_layout () plt . show ()","title":"Single prediction"},{"location":"examples/forecast/#multiple-predictions","text":"predictions_test = [] for i in range ( 1000 ): Y_pred_test = model . sample ( X_AR_test ) # store predictions predictions_test . append ( Y_pred_test [:, - 1 ]) predictions_test = np . array ( predictions_test ) plt . figure ( figsize = ( 12 , 4 )) # mean prediction plt . plot ( predictions_test . mean ( axis = 0 ), label = 'Prediction (mean)' , alpha = 1. , color = '0' ) # 90% CI xx = np . arange ( len ( Y_pred_test )) y_top = np . quantile ( predictions_test , q = 0.95 , axis = 0 ) y_bot = np . quantile ( predictions_test , q = 0.05 , axis = 0 ) plt . fill_between ( xx , y_bot , y_top , label = \"90% CI\" , alpha = 0.5 , color = 'C0' ) # true plt . plot ( Y_AR_test [:, - 1 ], label = 'True' , alpha = 1. , color = 'C1' ) plt . legend ( loc = 'best' ) plt . tight_layout () plt . show ()","title":"Multiple predictions"},{"location":"examples/metrics/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Metrics usage This example shows how to use metrics to measure the quality of the generated samples. The metrics take two samples (real and generated) and estimate discrepancies between their distributions. The smaller metrics values the better. Generate two samples Let's generate two samples from multivariate normal distributions with the same covariance matrices, but with some distance between their means. import matplotlib.pyplot as plt import numpy as np from probaforms import metrics def gen_two_samples ( dist , N ): sigma = np . array ([[ 1 , 0.7 ], [ 0.7 , 1 ]]) mu_x = np . array ([ 0 , 0 ]) mu_y = mu_x + dist / np . sqrt ( 2 ) X = np . random . multivariate_normal ( mu_x , sigma , N ) Y = np . random . multivariate_normal ( mu_y , sigma , N ) return X , Y # generate two samples with a size of 1000 dist = 2 X , Y = gen_two_samples ( dist , N = 1000 ) def plot_samples ( X , Y , dist = 0 ): plt . figure ( figsize = ( 6 , 4 )) plt . scatter ( X [:, 0 ], X [:, 1 ], label = 'X' , alpha = 0.5 , color = 'C0' ) plt . scatter ( Y [:, 0 ], Y [:, 1 ], label = 'Y' , alpha = 0.5 , color = 'C1' ) plt . title ( \"Distance = %.f\" % ( dist )) plt . legend () plt . tight_layout () plt . show () plot_samples ( X , Y , dist ) Compute discrepancies between the samples def get_metrics ( X , Y ): mu , sigma = metrics . frechet_distance ( X , Y ) print ( r \"Frechet Distance = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . kolmogorov_smirnov_1d ( X , Y ) print ( r \"Kolmogorov-Smirnov = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . cramer_von_mises_1d ( X , Y ) print ( r \"Cramer-von Mises = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . anderson_darling_1d ( X , Y ) print ( r \"Anderson-Darling = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . roc_auc_score_1d ( X , Y ) print ( r \"ROC AUC = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . kullback_leibler_1d_kde ( X , Y ) print ( r \"Kullback-Leibler KDE = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . jensen_shannon_1d_kde ( X , Y ) print ( r \"Jensen-Shannon KDE = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . maximum_mean_discrepancy ( X , Y ) print ( r \"Maximum Mean Discrepancy = %.6f +- %.6f \" % ( mu , sigma )) get_metrics ( X , Y ) Frechet Distance = 3.711472 +- 0.240218 Kolmogorov-Smirnov = 0.509375 +- 0.015197 Cramer-von Mises = 64.544793 +- 3.777022 Anderson-Darling = 423.161173 +- 20.731482 ROC AUC = 0.824993 +- 0.007886 Kullback-Leibler KDE = 0.852085 +- 0.063555 Jensen-Shannon KDE = 0.173695 +- 0.009267 Maximum Mean Discrepancy = 0.305732 +- 0.019054 Additional experiments with other distances dist = 10. X , Y = gen_two_samples ( dist , N = 1000 ) plot_samples ( X , Y , dist ) get_metrics ( X , Y ) Frechet Distance = 100.640957 +- 1.078155 Kolmogorov-Smirnov = 1.000000 +- 0.000000 Cramer-von Mises = 166.667082 +- 0.000017 Anderson-Darling = 1015.172596 +- 0.022680 ROC AUC = 1.000000 +- 0.000000 Kullback-Leibler KDE = 12.633292 +- 0.031014 Jensen-Shannon KDE = 0.692143 +- 0.000470 Maximum Mean Discrepancy = 1.502160 +- 0.007987 dist = 0. X , Y = gen_two_samples ( dist , N = 1000 ) plot_samples ( X , Y , dist ) get_metrics ( X , Y ) Frechet Distance = 0.007647 +- 0.005009 Kolmogorov-Smirnov = 0.053815 +- 0.010366 Cramer-von Mises = 0.325126 +- 0.186177 Anderson-Darling = 0.955208 +- 0.973257 ROC AUC = 0.513250 +- 0.005923 Kullback-Leibler KDE = 0.011113 +- 0.003471 Jensen-Shannon KDE = 0.002718 +- 0.000755 Maximum Mean Discrepancy = 0.001662 +- 0.001076","title":"Metrics usage"},{"location":"examples/metrics/#metrics-usage","text":"This example shows how to use metrics to measure the quality of the generated samples. The metrics take two samples (real and generated) and estimate discrepancies between their distributions. The smaller metrics values the better.","title":"Metrics usage"},{"location":"examples/metrics/#generate-two-samples","text":"Let's generate two samples from multivariate normal distributions with the same covariance matrices, but with some distance between their means. import matplotlib.pyplot as plt import numpy as np from probaforms import metrics def gen_two_samples ( dist , N ): sigma = np . array ([[ 1 , 0.7 ], [ 0.7 , 1 ]]) mu_x = np . array ([ 0 , 0 ]) mu_y = mu_x + dist / np . sqrt ( 2 ) X = np . random . multivariate_normal ( mu_x , sigma , N ) Y = np . random . multivariate_normal ( mu_y , sigma , N ) return X , Y # generate two samples with a size of 1000 dist = 2 X , Y = gen_two_samples ( dist , N = 1000 ) def plot_samples ( X , Y , dist = 0 ): plt . figure ( figsize = ( 6 , 4 )) plt . scatter ( X [:, 0 ], X [:, 1 ], label = 'X' , alpha = 0.5 , color = 'C0' ) plt . scatter ( Y [:, 0 ], Y [:, 1 ], label = 'Y' , alpha = 0.5 , color = 'C1' ) plt . title ( \"Distance = %.f\" % ( dist )) plt . legend () plt . tight_layout () plt . show () plot_samples ( X , Y , dist )","title":"Generate two samples"},{"location":"examples/metrics/#compute-discrepancies-between-the-samples","text":"def get_metrics ( X , Y ): mu , sigma = metrics . frechet_distance ( X , Y ) print ( r \"Frechet Distance = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . kolmogorov_smirnov_1d ( X , Y ) print ( r \"Kolmogorov-Smirnov = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . cramer_von_mises_1d ( X , Y ) print ( r \"Cramer-von Mises = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . anderson_darling_1d ( X , Y ) print ( r \"Anderson-Darling = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . roc_auc_score_1d ( X , Y ) print ( r \"ROC AUC = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . kullback_leibler_1d_kde ( X , Y ) print ( r \"Kullback-Leibler KDE = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . jensen_shannon_1d_kde ( X , Y ) print ( r \"Jensen-Shannon KDE = %.6f +- %.6f \" % ( mu , sigma )) mu , sigma = metrics . maximum_mean_discrepancy ( X , Y ) print ( r \"Maximum Mean Discrepancy = %.6f +- %.6f \" % ( mu , sigma )) get_metrics ( X , Y ) Frechet Distance = 3.711472 +- 0.240218 Kolmogorov-Smirnov = 0.509375 +- 0.015197 Cramer-von Mises = 64.544793 +- 3.777022 Anderson-Darling = 423.161173 +- 20.731482 ROC AUC = 0.824993 +- 0.007886 Kullback-Leibler KDE = 0.852085 +- 0.063555 Jensen-Shannon KDE = 0.173695 +- 0.009267 Maximum Mean Discrepancy = 0.305732 +- 0.019054","title":"Compute discrepancies between the samples"},{"location":"examples/metrics/#additional-experiments-with-other-distances","text":"dist = 10. X , Y = gen_two_samples ( dist , N = 1000 ) plot_samples ( X , Y , dist ) get_metrics ( X , Y ) Frechet Distance = 100.640957 +- 1.078155 Kolmogorov-Smirnov = 1.000000 +- 0.000000 Cramer-von Mises = 166.667082 +- 0.000017 Anderson-Darling = 1015.172596 +- 0.022680 ROC AUC = 1.000000 +- 0.000000 Kullback-Leibler KDE = 12.633292 +- 0.031014 Jensen-Shannon KDE = 0.692143 +- 0.000470 Maximum Mean Discrepancy = 1.502160 +- 0.007987 dist = 0. X , Y = gen_two_samples ( dist , N = 1000 ) plot_samples ( X , Y , dist ) get_metrics ( X , Y ) Frechet Distance = 0.007647 +- 0.005009 Kolmogorov-Smirnov = 0.053815 +- 0.010366 Cramer-von Mises = 0.325126 +- 0.186177 Anderson-Darling = 0.955208 +- 0.973257 ROC AUC = 0.513250 +- 0.005923 Kullback-Leibler KDE = 0.011113 +- 0.003471 Jensen-Shannon KDE = 0.002718 +- 0.000755 Maximum Mean Discrepancy = 0.001662 +- 0.001076","title":"Additional experiments with other distances"},{"location":"examples/regression/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Regression Generate data set In this example, we will solve a regression task with uncertainty estimation using Conditional Normalizing Flows. In addition, we will compare this approach with the Gaussian Process. Let\u2019s generate a data set with observations sampled from a normal distribution, where mean and variance depend on input variable X. # import of basic libraries import matplotlib.pyplot as plt from matplotlib import cm import pandas as pd import numpy as np # function we would like to predict def func ( X ): return np . exp ( - X ) # input variable X = np . linspace ( 0 , 5 , 500 ) . reshape ( - 1 , 1 ) # mean values of targets y mu = func ( X ) # normal noise eps = np . random . normal ( 0 , 1 , X . shape ) sigma = 0.05 * ( X + 0.5 ) # target variable we need to predict y = mu + eps * sigma plt . figure ( figsize = ( 12 , 6 )) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show () Normalizing flows Now, we use Conditional Real NVP to learn conditional distribution p(y|X) of the observations. Here, X is condition, y is our target to learn its distribution. from probaforms.models import RealNVP # fit nomalizing flow model model = RealNVP ( lr = 0.01 , n_epochs = 100 ) model . fit ( y , X ) # (target, condition) # sample new observations y_gen = model . sample ( X ) The figure below shows that Normalizing Flow (NF) successfully learnt the distribution p(y|X) . We can use NF to sample new objects from the distribution that are similar to the real observations. plt . figure ( figsize = ( 12 , 6 )) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y_gen , marker = '+' , label = 'Generated with NF' , color = 'C1' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , linewidth = 2 ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show () Now, we will repeat the sampling procedure several times and empirically estimate the mean value of the function and its standard deviation. The standard deviation we consider as the prediction uncertainty. The figure below demonstrates that NF successfully estimates the mean and standard deviation of the sample. y_preds = [] # repeat sampling several times for i in range ( 1000 ): # sample with NF y_gen = model . sample ( X ) y_preds . append ( y_gen ) y_preds = np . array ( y_preds ) # estimate the mean of the predictions mu_pred = y_preds . mean ( axis = 0 ) . reshape ( - 1 ,) # estimate the standard deviation of the predictions sigma_pred = y_preds . std ( axis = 0 ) . reshape ( - 1 ,) plt . figure ( figsize = ( 12 , 6 )) plt . fill_between ( X [:, 0 ], y1 = mu_pred + sigma_pred , y2 = mu_pred - sigma_pred , color = 'C1' , alpha = 0.5 , label = r '$\\mu \\pm \\sigma$ by NF' ) plt . plot ( X , mu_pred , label = 'Mean by NF' , color = 'C1' , linewidth = 4 ) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , color = 'C0' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show () Gaussian Process Finally, let\u2019s solve the same task with the Gaussian Process (GP) to compare the results. The plot below shows that GP predicts the mean with good precision. However, it is not able to learn the dependency of the standard deviation from the input variable X. from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import WhiteKernel , RBF , ConstantKernel as C # define kernel kernel = C () * RBF () + WhiteKernel () # fit GP gpr = GaussianProcessRegressor ( kernel = kernel ) gpr . fit ( X , y ) # make predictions of the means and standard deviations mu_pred_gp , sigma_pred_gp = gpr . predict ( X , return_std = True ) plt . figure ( figsize = ( 12 , 6 )) plt . fill_between ( X [:, 0 ], y1 = mu_pred_gp + sigma_pred_gp , y2 = mu_pred_gp - sigma_pred_gp , color = 'C1' , alpha = 0.5 , label = r '$\\mu \\pm \\sigma$ by GP' ) plt . plot ( X , mu_pred_gp , label = 'Mean by GP' , color = 'C1' , linewidth = 4 ) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , color = 'C0' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show ()","title":"Regression"},{"location":"examples/regression/#regression","text":"","title":"Regression"},{"location":"examples/regression/#generate-data-set","text":"In this example, we will solve a regression task with uncertainty estimation using Conditional Normalizing Flows. In addition, we will compare this approach with the Gaussian Process. Let\u2019s generate a data set with observations sampled from a normal distribution, where mean and variance depend on input variable X. # import of basic libraries import matplotlib.pyplot as plt from matplotlib import cm import pandas as pd import numpy as np # function we would like to predict def func ( X ): return np . exp ( - X ) # input variable X = np . linspace ( 0 , 5 , 500 ) . reshape ( - 1 , 1 ) # mean values of targets y mu = func ( X ) # normal noise eps = np . random . normal ( 0 , 1 , X . shape ) sigma = 0.05 * ( X + 0.5 ) # target variable we need to predict y = mu + eps * sigma plt . figure ( figsize = ( 12 , 6 )) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show ()","title":"Generate data set"},{"location":"examples/regression/#normalizing-flows","text":"Now, we use Conditional Real NVP to learn conditional distribution p(y|X) of the observations. Here, X is condition, y is our target to learn its distribution. from probaforms.models import RealNVP # fit nomalizing flow model model = RealNVP ( lr = 0.01 , n_epochs = 100 ) model . fit ( y , X ) # (target, condition) # sample new observations y_gen = model . sample ( X ) The figure below shows that Normalizing Flow (NF) successfully learnt the distribution p(y|X) . We can use NF to sample new objects from the distribution that are similar to the real observations. plt . figure ( figsize = ( 12 , 6 )) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y_gen , marker = '+' , label = 'Generated with NF' , color = 'C1' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , linewidth = 2 ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show () Now, we will repeat the sampling procedure several times and empirically estimate the mean value of the function and its standard deviation. The standard deviation we consider as the prediction uncertainty. The figure below demonstrates that NF successfully estimates the mean and standard deviation of the sample. y_preds = [] # repeat sampling several times for i in range ( 1000 ): # sample with NF y_gen = model . sample ( X ) y_preds . append ( y_gen ) y_preds = np . array ( y_preds ) # estimate the mean of the predictions mu_pred = y_preds . mean ( axis = 0 ) . reshape ( - 1 ,) # estimate the standard deviation of the predictions sigma_pred = y_preds . std ( axis = 0 ) . reshape ( - 1 ,) plt . figure ( figsize = ( 12 , 6 )) plt . fill_between ( X [:, 0 ], y1 = mu_pred + sigma_pred , y2 = mu_pred - sigma_pred , color = 'C1' , alpha = 0.5 , label = r '$\\mu \\pm \\sigma$ by NF' ) plt . plot ( X , mu_pred , label = 'Mean by NF' , color = 'C1' , linewidth = 4 ) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , color = 'C0' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show ()","title":"Normalizing flows"},{"location":"examples/regression/#gaussian-process","text":"Finally, let\u2019s solve the same task with the Gaussian Process (GP) to compare the results. The plot below shows that GP predicts the mean with good precision. However, it is not able to learn the dependency of the standard deviation from the input variable X. from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import WhiteKernel , RBF , ConstantKernel as C # define kernel kernel = C () * RBF () + WhiteKernel () # fit GP gpr = GaussianProcessRegressor ( kernel = kernel ) gpr . fit ( X , y ) # make predictions of the means and standard deviations mu_pred_gp , sigma_pred_gp = gpr . predict ( X , return_std = True ) plt . figure ( figsize = ( 12 , 6 )) plt . fill_between ( X [:, 0 ], y1 = mu_pred_gp + sigma_pred_gp , y2 = mu_pred_gp - sigma_pred_gp , color = 'C1' , alpha = 0.5 , label = r '$\\mu \\pm \\sigma$ by GP' ) plt . plot ( X , mu_pred_gp , label = 'Mean by GP' , color = 'C1' , linewidth = 4 ) plt . plot ( X , mu , label = 'Mean true' , color = '0' , linewidth = 2 ) plt . scatter ( X , y , marker = '+' , label = 'Observations' , color = 'C0' , linewidth = 2 ) plt . plot ( X , mu + sigma , label = r '$\\mu \\pm \\sigma$ True' , color = '0' , linewidth = 2 , linestyle = '--' ) plt . plot ( X , mu - sigma , color = '0' , linewidth = 2 , linestyle = '--' ) plt . xlabel ( \"X\" ) plt . ylabel ( \"y\" ) plt . grid () plt . legend () plt . show ()","title":"Gaussian Process"}]}